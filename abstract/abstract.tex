{\Huge Abstract}
\vspace{1cm}
% Field of research
% A brief motivation for the work
% What the research topic is
% Research approach(es) applied
% contributions

Research has recently shown a keen interest in database technology for analytical workloads that enables high-performance analysis and on-the-fly aggregation, where the main motivation is \bi~and\bd~products. Such products store data in main memory as compressed columns to maximize memory utilization and CPU throughput. \mde, a discipline that aims to increase developer productivity through the use of models on a higher level of abstraction, automates many of the complex programming tasks, like persistence and interoperability. One such product, \gap, has evolved over time and become a powerful and expressive tool for rapid application development. However, operations that process and analyze large amounts of data are slow, and the platform has a high memory footprint, mainly because no particular attention has been paid to storage format and structures in the source code. Based on the observation that \gap~has many similarities with an in-memory database, we are motivated to investigate if the challenges in \gap~can be overcome by applying techniques used in read-optimized databases. 

In this research, we enhance data representation, implement column storage with dictionary encoding and bitpacking in \gap~to reduce memory footprint and increase the platform ability to handle and analyze large datasets. We identify core operations that can exploit the new storage format, like join and filter operations. We test our implementation using a standard benchmark for analytical workloads while monitoring that transactional performance is not negatively affected. 

In \gap, column storage with dictionary encoding, bitpacking, and null-pointer compression leads to a memory reduction of 67 \% and a load time reduction of 36 \% for the \textit{TPC-H Q1 Data Load Benchmark}. Also, operations that are adjusted to utilize the column storage format sees a performance impact of one, two, and even three orders of magnitude compared to the original implementation. The new internal data representation in \gap~does not significantly reduce transactional performance. Thus, by using \gap~as a proof-of-concept, we have shown how techniques used by read-optimized databases increase \mde~versatility by enabling such tools to handle and analyze large datasets.


%Column storage alone has lead to a memory reduction of about 70\% in certain test cases. In addition to this, several operations have had performance improvements of 100X-1000X, including filter operations and index generation. We have been able to isolate a small set of operations that can be used as tools for efficient programs. We have proved that database technology can be used in a model-driven development tool, something that has not been done before.

% With the advent of Big Data and \bd~products, database technology for high-performance analysis of data has become increasingly more popular. Data is commonly stored as columns for easy aggregation of data, better compression, and better CPU and cache performance. \mdd~has been around for decades, but has not gained particular traction until now. \gap~is one such platform. This system has evolved for roughly twenty years into becoming a powerful tool for rapid application development. Although most of the data in \gap~are persisted in relational databases (like \oracle, \mssql, \mysql), the data processing happens in-memory. No particular attention has been paid to storage formats and structures, data has been stored in whichever structures that are the most convenient for the developes of \gap. This research investigates which technologies from the research field of databases that can be applied to the field of \mdd, and which techniques from \mdd that are promising for the data field.
