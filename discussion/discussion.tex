\chapter{Discussion}
\label{chap:Discussion}
In this chapter, we discuss our findings in a broader perspective. We study the implications of our research for \gap, \mde~in general, and for traditional programming languages. We also provide a section on limitations and critics.

\clearpage

\section{Implications for Genus App Platform}
\label{sec:Implications for Genus App Platform}
So far in this research, we have reduced memory consumption, improved load time, and increased operation performance for our benchmarks. Even though these benchmarks only test a small subset of \gap~functionality, we believe the advantages will appear throughout the platform. Even without dictionary compression and the column selector functionality discussed in Section \ref{sec:Column Selection}, storing data in columns, either as \cn{GValue}s or primitive data types, is a good idea. However, we also expect to see functionality in \gap~slowed down by our column store implementation. This is likely to happen on operations that are not adapted to the new storage format; operations that heavily rely on \cn{GValue}s and row storage, like the filter operation in Benchmark \ref{bm:filter}.

In the thesis introduction, we said \genus~have focused on keeping the source code readable and maintainable. However, as we have seen, this emphasis have also resulted in performance issues in \gap. With the introduction of \cn{CompositionValueCollection}, the code is more slightly complicated and less readable. Still, the \cn{CompositionObject} class is intact and works exactly as before, and it has a \cn{GValue} interface. In other words, the application works like before, the \cn{GetValue} and \cn{SetValue} on \cn{CompositionObject} have only got a new implementation. In addition, the interface to the column store, or \cn{CompositionValueCollection}, is clean, and requires \cn{CompositionFieldDescriptor} or data source index, or both, to access data.

We believe further development in \gap~may proceeds as before. However, two things should be kept in mind. First, developers must be aware the cost of \cn{GValue} allocation when getting values from a \cn{CompositionObject}. Thus, read-intensive operations like source measure lookup in the \textit{TPC-H Q1 Data Load Benchmark} performs poorly if no special attention is given. Second, developers should identify new operations or optimizations that benefit from the column data structure. These two things are intertwined; read-intense operations that allocate \cn{GValue}s frequently are also likely to benefit from column store exploitation.

\section{Implications for Model-Driven Engineering}
\label{sec:Implications for Model-Driven Engineering}
This research does not provide an extensive overview of \mdd-tools and their particular challenges, but we believe that their ability to handle and analyze large datasets is commonly outperformed by hand-made, traditional programs. Our research shows that by applying database technology used in read-optimized databases, \mde-tools can gain this ability. We have shown that internal data representation is key for low memory usage and high performance, also in \mdd. Clever data structures increase \mde~versatility and enable new functionality, for instance \bd.

We have also discovered that changing internal data representation only affects the method engineering layer in \mde. Modelers in the information system development layer do not need to be affected by the underlying storage technology. In other words, object classes and other data models are still expressed such that they are close to the business domain, while the underlying \mde~infrastructure optimizes data representation. This decoupling is different from a traditional programming language, where class definitions serve as both business problem abstractions and data containers.

\ffigure{img/oracle-dual.png}{The \oracle~dual format. Data is stored as both rows for transactional performance, and columns for analytical performance. (Adapted from \cite{Oracle2015-fs})}{fig:oracle-dual}
There exist many different \mdd-tools with various approaches. Some are simple code-generation tools, while other provides an entire infrastructure. For layered systems that loads and process data in-memory, like \gap, there is much potential in data structure optimization: Different tasks in the platform should use whichever format is better-suited for that particular task. In Section \ref{sec:Mixed Workloads}, we read abut mixed workloads and that consistency, correctness, and data freshness can be sacrificed for performance. We also know that row-wise storage normally outperforms columns on transactional workloads. \mde~tools may have it all. For instance, \bd~funcionality should load data as immutable, read-optimized, and compressed columns while write-intense should use structures that store data in rows. This is similar to the \oracle~dual format, depicted in Figure \ref{fig:oracle-dual}.

\section{Implications for Traditional Programming Languages}
\label{sec:Implications for Traditional Programming Languages}
In \mde, we have changed the internal data representation withoud affecting the modelers in the information system development layer. However, as far as we are aware, traditional programming languages do not offer the same functionality, as classes serve the role of \textit{both a data structure as well as a business abstraction}. Although developers are free to move class attributes into database-inspired structures, like columns, this degrades code readability: Data gets decoupled from the objects they belong to and class definitions no longer contain available properties. The implementation of the interal data structures is also tedious, which increas application development time.

\begin{pythoncode}{Decorators in Python that specifies storage engine.}{lst:python-decorator}
class Person:
    @storage(type='column')
    first_name = null

    @storage(type='column')
    last_name = null

    @storage(type='dictionary')
    gender = null
\end{pythoncode}
We believe even traditional programming languages can benefit from the techniques we have studied in this research. For instance, the Python programming language implements the \textit{decorator pattern} \cite{noauthor_undated-aq} which dynamically changes the source code of a function or attribute. We believe decorators can be used to configure the underlying storage engine for class properties. Listing \ref{lst:python-decorator} exemplifies a \cn{Person} class where first and last name are stored in regular columns, while a dictionary encoded column stores the person's gender.

In section \ref{sub:Null-Pointer Compression}, we saw how null-pointer compression significantly reduced the number of bytes needed to store a \cn{CompositionObject}. We believe traditional programming languages could leverage this technique by not allocating buffers for pointers before they are set.


\section{Limitations and Critics}
\label{sec:Limitations and Critics}
There are several limitations in our research. First of all, is the testing of the framework of . We have only tested a limited subset of the functionality to show the potential. We have tested write performance, but this test is limited in scope. Due to time constraints, all tests were run only three times. Ideally, they should be run more to gain a statistical foundation. At last, we have run all tests in debug-mode. We are unaware of what changes this gives in production. Despite of this, we believe our results serve to show the potential in our techniques. We have measured large differences, differences that are likely to be apperant in non-debugger compiled code.

The second limitation, is that we have based our entire research on \gap. However \mde~supporting infrastructures comes in all sizes and shapes. Thus, our changes might not be appliccable to all systems. There might be systems out there solving these problems more elegantly, by custom code, or for instance are more connected with a database. Nevertheless, we still believe that our modifications to \gap~serves as a proof-of-concept for \mde-tools challenged with performance and data layout issues.

This research has focused on read-optimized databases, which is because this read-operations is where \gap~had the largest performance issues. However, we did not investigate technologies for write-optimized, or OLTP databases. There is absoultely much to get from this, and are convinced that certain \gap~oprations would benefit from such row storage. However, as we have seen, data update operations are dominated by other operations, like integrety checks and database persistence. However, as we have seen, most operations work on an entire column at a time, rather than an entire row. According to our results, no operations have been hurt by the column store. We do, however, believe that there are operations that would benefit from a row store.

