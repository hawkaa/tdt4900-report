\chapter{Benchmarks}
\label{app:bm}

In this appendix, we elaborate on the benchmarks run to measure performance in this research. There are five benchmarks in total, all aiming to test different parts of \gap~where memory usage or performance might have been affected. The last benchmark, Benchmark \ref{bm:odin-load}, is based on real customer data from one of \genus' customers.

We measure memory usage for various states in the program by querying the built-in \name{FastMM} memory manager. To measure the entire \gap~memory footprint, including the .NET layer and the graphical user interface, we use the built-in memory manager in the \name{Windows} operating system. 

We run all benchmarks in \textit{Genus Desktop} in \delphi~debugger mode. Test output and debugging statements are extracted through \textit{Trace Loge}, a logging tool developed by \genus~for their applications. Before running any benchmark, \textit{Genus Desktop} and the debugger was restarted such that the effects of caching are minimized.

All tests are run on a Windows 10 Dell workstation with a 64-bit, 2.40 GHz Intel\textregistered Xeon\textregistered E5620 processor, and 8.00 GB RAM. The \delphi~version used is 23.0.21418.4207. 


\section{Data Mart Load Benchmark}
\label{bm:q1}
In this benchmark, we measure analytical performance in \gap~using using a benchmark inspired by and based on the Q1 in the TPC-H Benchmark. As we saw in \ref{sec:Testing OLAP Databases}, the TPC-H benchmark is made for decision support workloads and consists of a suite of business oriented ad-hoc queries. The schema consists of eight separate tables, as seen in Figure \ref{fig:tpc-h}. The table columns have a variety of different data types, including integers, floating point numbers, variable and fixed width string, identifiers, and booleans.

\afigure{img/tpc-h-q1.png}{TPC-H Q1 query. (Adapted from \cite{Transaction_Processing_Performance_Council_TPC2014-ux})}{fig:tpc-h-q1}{0.75}

Q1, or the \textit{Pricing Summary Report Query}, provides a summary of all items shipped within a date range \cite{Transaction_Processing_Performance_Council_TPC2014-ux}. It aggregates price with and without discount, and with and without tax, as well as average quantity, price, and discount. The results are grouped by \texttt{RETURNFLAG} and \texttt{LINESTATUS}. The original query is shown in Figure \ref{fig:tpc-h-q1}.

\afigure{img/gap-data-mart.png}{Data mart for the \textit{Data Mart Load Benchmark}.}{fig:gap-data-mart}{0.7}

\afigure{img/tpc-h-q1-ui.png}{\textit{Data Mart Load Benchmark} user interface in \gd.}{fig:tpc-h-q1-ui}{1.0}

To accommodate our needs for this research, as well as adapt the query to fit \gap~and \gd, we perform some modifications. First, \texttt{RETURNFLAG}, \texttt{LINESTATUS}, and \texttt{SHIPDATE} are extracted as code domains such that they can be used as dimensions in the data mart. Also, we remove unnecessary table columns irrelevant to the query from the mart. The resulting data mart is shown in Figure \ref{fig:gap-data-mart}. Second, since \gd~does not work with an SQL interface, but rather with a user interface with graphs and boxes, we translated the query to a reporting dashboard with bar charts and selection boxes. The resulting user interface is shown in Figure \ref{fig:tpc-h-q1-ui}. 

We populated the database with the \texttt{dbgen} tool provided by the \name{Transaction Processing Performance Council}. However, since the default scaling factor for the TPC-H benchmark is too large for \gap, we created two new dataset sizes, SF1 and SF10, with 1 \% and 10 \% of the original data respectively. The data was downscaled by keeping every 10th or 100th row in \texttt{LINEITEM} and removing all orders, suppliers, customers, and parts that no longer were connected to any elements in this table. 

We focus our testing on time and memory footprint using to load this data mart and corresponding GUI into \gd~rather than measuring the time it takes to answer each query. The reason for this is that we have not looked at \gd~specific optimizations in this research, only modifications that this component can use reduce memory footprint and decrease load time. Loading a data mart takes the majority of the time anyway, and once the data mart is loaded, the application answers queries efficiently.

\subsection{Test Input}
\label{sub:Test Input}

Two different scaling factors: SF1 with 60,000 \texttt{LINEITEM} rows, and SF10 with 600,000 \texttt{LINEITEM} rows.

\subsection{Test Output}
\label{sub:Test Output}

We list the benchmark's output below. The output is recorded in the time between the shortcut button for the \gd~instance is pressed until the user interface (Figure \ref{fig:tcp-h-q1-ui}) is displayed on the screen.

\paragraph{Bytes Per \texttt{LINEITEM}}

The number of bytes allocated per \texttt{LINEITEM} in the memory manager is reported. We measure memory usage before and after the \texttt{LINEITEM} data source load and then divide by the number of loaded items in the data source.

\paragraph{Load Time}

The benchmark reports the time it takes for the \texttt{LINEITEM} data source to be loaded. The output disregards the overhead in creating composition objects and parsing XML from the underlying database infrastructure. Measurements are reported in milliseconds.

\paragraph{Lookup Index Generation (Join)}

This output shows how many milliseconds it takes to create lookup indexes between tables in the data mart. This operation is considered as a join operation. A join is performed on all relations in the mart, which is the \texttt{LINESTATUS}, \texttt{RETURNFLAG}, and \texttt{SHIPDATE} in the \texttt{LINEITEM} table. A measurement is made for every relation. Creating a lookup for \texttt{SHIPDATE} takes longer than the two others because it is a larger join and dates are harder to parse.

\paragraph{Source Measure Lookup}

In \gd, all calculations and aggregations are performed on arrays of double precision floating point values. Hence, after all data for a given data source has been loaded, such arrays are extracted for all measures in the data mart. In the \tpchdl, such structures are created for integer column \texttt{QUANTITY} and floating point columns \texttt{EXTENDEDPRICE}, \texttt{DISCOUNT}, and \texttt{TAX}. For each column, the number of milliseconds used to create this lookup is measured.

\paragraph{Total Memory Footprint}
\label{par:Total Memory Footprint}

We measure the total memory footprint of the \tpchdl~\gd~application by using the Windows 10 memory manager such that all overhead in the .NET and user interface layers is included. The \textit{In Use} memory metric is recorded before the \gd~instance is loaded and when the UI is shown to the user. Since the measurement includes more than our application and are affected by background tasks and other running program, the total memory footprint output is not very accurate. However, it serves as a confirmation on other memory measurements, as well as an indication that total application memory consumption is reduced.

\section{Write Benchmark}
\label{bm:write}

\afigure{img/lineitem.png}{\texttt{LINEITEM} table layout. (Adapted from \cite{Transaction_Processing_Performance_Council_TPC2014-ux})}{fig:lineitem}{1.0}

We test write performance in \gap~to see that it is not negatively affected by the changes we make in this research. We base our benchmark on the \lineitem~table in the TPC-H benchmark. The main reason for using this table is the availability of different data types, which we see in Figure \ref{fig:lineitem}.

The benchmark reads $N$ items into a data source, for all the elements, the following fields are modified:
\begin{itemize}
    \item \texttt{QUANTITY = QUANTITY + 1} to measure integer performance.
    \item \texttt{EXTENDEDPRICE = EXTENDEDPRICE + 1.0} to measure floating point performance.
    \item \texttt{COMMENT = COMMENT + "1"} to measure string performance.
    \item \texttt{SHIPDATE = SHIPDATE PLUS 1 DAYS} to measure date performance.
\end{itemize}

\afigure{img/write-test-definition.png}{The task definition for the Write Benchmark. There is one loop per attribute.}{fig:write-test-definition}{0.4}

We create one loop per property, such that there is a total of four loops in this task. The full task definition is seen in Figure \ref{fig:write-test-definition}.

\subsection{Test Input}
\label{sub:Test Input}
The number of \texttt{LINEITEM}s loaded into the data source and modified, which is specified in a dialog box before the task is run. For all tests performed in this research, we use 1000 elements as input.

\subsection{Test Output}
\label{sub:Test Output}
For each property modified, the task reports the time it takes to enumerate the entire data source and modify the property. In addition to total time, the average time per modification is listed as well.

\section{Filter Benchmark}
\label{bm:filter}
\begin{table}
    \centering
    \begin{tabular}{c | c}
        Predicate & Selects (of 100,000) \\
        \hline
        \hline
        \texttt{LINESTATUS = F} & 49,864 \\
        \texttt{TAX >= 0.02} & 66,851 \\
        \texttt{EXTENDEDPRICE >= 1000} & 99,956 \\
        \texttt{EXTENDEDPRICE >= 40000} & 45,408\\
        \texttt{EXTENDEDPRICE < 1000} & 44 \\
        \texttt{EXTENDEDPRICE = 42995.94} & 1 \\
        \texttt{SHIPDATE IN 1995} & 15,435 \\
        \texttt{SHIPINSTRUCT\_NULL HAS VALUE} & 74,991 \\
    \end{tabular}
    \caption{Predicates in the \textit{Filter Benchmark}. The second column indicated selectivity by showing the number of selected rows (of 100,000 total).}
    \label{tab:Filter Test}
\end{table}

To test \gap's ability to move data from one data source to another based on a filter, we establish a benchmark based on the \texttt{LINEITEM} table from the TPC-H benchmark. As for the \textit{Write Benchmark}, this test is used for the various data types available.

A list of predicates which compose the benchmark, and objects that satisfy each predicate are moved from the original data source to a filtered data source. We select the predicates to span a variety of different data types, selectivities, and operators. Since there are no \null~columns in \texttt{LINEITEM}, we create a new column \texttt{SHIPINSTRUCT\_NULL} and assign \null~to all cells with the value \textit{NONE}. We list the different predicates and respective selectivities in Table \ref{tab:Filter Test}.

\subsection{Test Input}
\label{sub:Test Input}
The number of \texttt{LINEITEM}s loaded into the original data source. We only use 100,000 as input in this research.

\subsection{Test Output}
\label{sub:Test Output}
The timing result for the filter operation for each predicate; the time it takes to move the objects from the original data source to the filtered data source. Reported in milliseconds.

\section{Data Mart Load Benchmark II}
\label{bm:odin-load}

\afigure{img/odin-load.png}{Data mart for the \textit{Data Mart Load Benchmark II}.}{fig:odin-load}{1.0}

We run the same benchmark as Benchmark \ref{bm:q1}, but with a subset of data from one of \genus' customers. The data mart for the \gd~analysis is composed of three tables; \texttt{FUND}, \texttt{CUSTOMER}, and \texttt{CUSTOMER BALANCE DAILY}, with 40, 442,715, and 203,519 elements respectively. \texttt{CUSTOMER BALANCE DAILY} links the two other tables by indicating how many shares each customer have in each fund. Figure \ref{fig:odin-load} shows the data mart definition for the benchmark.

The reason why we choose to introduce another data mart load benchmark is that this benchmark contains a larger join than the \tpchdl. In this benchmark, the lookup index generation operation must create a lookup index between \texttt{CUSTOMER} and \texttt{CUSTOMER BALANCE DAILY}, which have 442,715 and 203,519 elements respectively.

\subsection{Test Input}
\label{sub:Test Input}
None.

\subsection{Test Output}
\label{sub:Test Output}
For each table, the bytes per element and load time is reported. In addition to this, the benchmark prints the time it takes to perform each of the two joins in the data mart.

%\section{OLTP Test}
%\label{sec:OLTP Test}
%Testing creating, updates, deletes, and everything.
%
%\subsection{Test Input}
%\label{sub:Test Input}
%
%\subsection{Test Output}
%\label{sub:Test Output}
%The time it takes to blabla bla
%
%
%\section{Real Customer Test: Shareholder Analysis}
%\label{sec:Real Customer Test: Shareholder Analysis}
%We also run some tests on some real test cases proposed by \genus. This motivated the entire research.
%
%Both test cases will be run and presented to the respective customers.
%
%Due to confidential information, the names and the specific product are not available in this report, and the respective companies are held anonymous.
%
%The first test case tests memory usage and load time for an analysis on stock ownership.
%\begin{itemize}
%    \item Distributor and Customer Reporting
%    \item Operations, Settlements and Fund Management Support
%    \item Payments and Payment matching
%    \item Commissions Management
%    \item Customer Relationship Management (CRM)
%    \item Portfolio and Company Management
%    \item Order processing
%\end{itemize}


%\subsection{Real Customer Data Test Case II: A Retail Chain}
%\label{sub:Real Customer Data Test Case II: A Retail Chain}
%\begin{itemize}
%    \item Manage operations for 2200 grocery stores
%    \item Corporate Performance Management (CPM)
%    \item Apps for Operational Control \& Monitoring
%    \item Employee- and customer surveys
%    \item Food Safety and Risk Management
%\end{itemize}
%Like above, this is tested for memory usage and load time.



