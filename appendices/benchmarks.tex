\chapter{Benchmarks}
\label{app:Benchmarks}
In this appendix, we elaborate on the benchmarks run to measure performance. There is a total of five tests, all aiming to test different parts of the application where the memory or time usage might have been affected. The two last benchmarks, Benchmark \ref{bm:odin-load} and Benchmark \ref{bm:odin-crud}, are run on real customer data from one of \genus' customers.

All tests are run on a Windows 10 Dell workstation with a 64-bit, 2.40 GHz Intel\textregistered Xeon\textregistered E5620 processor and 8.00 GB RAM. The \delphi~version used is 23.0.21418.4207. 

Memory usage is measured for various states in the program by querying the built-in \name{FastMM} memory manager, and the values returned are used to measure memory deltas. To measure the entire application footprint, including the .NET layer and the graphical user interface, the built-in memory manager in Windows 10 is used.

We run all benchmarks in \textit{Genus Desktop} in \delphi~debugger mode. Test output and debugging statements is extracted through \textit{Trace Log}, a log tool developed by \genus~for their applications. For all benchmarks, a restart of the application is performed prior to running the test. This way, we minimize any effects caused by caching.

\section{TPC-H Q1 Data Load Benchmark}
\label{bm:q1}
In this benchmark, we measure analytical performance in \gap~using Q1 in the TPC-H Q1 benchmark. As we saw in \ref{sec:Testing}, the TPC-H benchmark is made for decision support workloads and consists of a suite of business oriented ad-hoc queries. The schema for the TPC-H benchmark consists of eight separate tables, as seen in Figure \ref{fig:tpc-h}. The table columns have a variety of different data types, including integers, floating points, variable and fixed width strings, identifiers, and booleans. 

\afigure{img/tpc-h-q1.png}{TPC-H Q1 query. Courtesy of \cite{Transaction_Processing_Performance_Council_TPC2014-ux}.}{fig:tpc-h-q1}{0.75}
Q1, or the \textit{Pricing Summary Report Query}, provides a summary of all items shipped within a date range \cite{Transaction_Processing_Performance_Council_TPC2014-ux}. It aggregates price with and without discount, and with and without tax, as well as average quantity, price, and discount. The results are grouped by \texttt{RETURNFLAG} and \texttt{LINESTATUS}. The original query is shown in Figure \ref{fig:tpc-h-q1}.

\afigure{img/gap-data-mart.png}{Data mart for Q1 in \tpch.}{fig:gap-data-mart}{0.7}
\afigure{img/tpc-h-q1-ui.png}{TPC-H Q1 query user interface in \gd.}{fig:tpc-h-q1-ui}{1.0}
Some adjustments were made to the query to fit \gap~and \gd. First, \texttt{RETURNFLAG}, \texttt{LINESTATUS}, and \texttt{SHIPDATE} had to be extracted as code domains such that they could be used as dimensions in the data mart. Additionally, the mart only contains the relevant object class properties that are required to answer the query. The resulting data mart is shown in Figure \ref{fig:gap-data-mart}. Second, since \gd~does not work with SQL, but rather an user interface with graphs and boxes, the query had to be translated into an user interface. The numbers returned by the query was displayed as bar charts grouped by \texttt{RETURNFLAG}. In this view, the user might select \texttt{SHIPDATE}, \texttt{LINESTATUS}, and \texttt{RETURNFLAG} to get the relevant data. The UI is shown in Figure \ref{fig:tpc-h-q1-ui}

Data for the benchmark was loaded using the \texttt{dbgen} tool provided by the \name{Transaction Processing Performance Council}. This tool populates the database with the amount of data that corresponds to scaling factor 1 (SF1), as seen in Figure \ref{fig:tpc-h-population}. However, this data size is too large for \gap~to handle, so two data sets with SF0.1 and SF0.01 were generated for this benchmark. The data reduction was performed by keeping every 10th or 100th row in \texttt{LINEITEM} and removing all orders, supppliers, customers, and parts that no longer was connected to any lineitems. Scaling factors less than 1 invalidates the TPC-H benchmark \cite{Transaction_Processing_Performance_Council_TPC2014-ux}, but this setup is still useful for us.

We focus our measurements on time and memory footprint used to load the TPC-H Q1 \gd~instance rather than the time it takes to answer the query. The reason for this is because of the scope of this research. We have not looked at \gd~specific optimizations, only operations that \gd~may use in load time to generate necessary structures and indexes. Load time takes the majority of the time, and once the mart is loaded, the application answers queries efficiently.

\subsection{Test Input}
\label{sub:Test Input}
SF0.01, SF0.1, and SF1 size data sets. Only SF0.01 and SF0.1 are used in this research.

\subsection{Test Output}
\label{sub:Test Output}
The benchmark's output is listed below. The output is recorded in the time between the shortcut button for the TPC-H Q1 \gd~instance is pressed until the user interface (Figure \ref{fig:tpc-h-q1-ui}) is displayed on the screen.

\paragraph{Bytes Per LINEITEM}
The number of bytes allocated per \texttt{LINEITEM} in the memory-manager is reported. Memory usage is measured before and after the \texttt{LINEITEM} data source load, and then divided by the number of loaded items.

\paragraph{Load Time}
\label{par:Load Time}
Load time for the \texttt{LINEITEM} data source is reported. This output only reports the time it takes to load data into composition objects, hence it disregards the overhead in creating composition objects and parsing XML files from the database backend. The result is reported in milliseconds.

\paragraph{Creation of Lookup Indexes}
This output shows how many milliseconds it takes to create lookup-indexes, or join the tables in the data mart. A join is performed all relations in the mart, which is \texttt{LINESTATUS}, \texttt{RETURNFLAG}, and \texttt{SHIPDATE}. The output represents the number of milliseconds it took to create each of the mentioned lookups. The \texttt{SHIPDATE} lookup takes longer, since date parsing and comparison is more complex and the date table is larger.

\paragraph{Source Measure Lookup Generation}
In \gd, all calculations and aggregations are performed on arrays of double precision floating point values. Hence, after all data for a given data source has been loaded, such array is extracted. In this benchmark, such arrays are created for integer column \texttt{QUANTITY} and floating point columns \texttt{EXTENDEDPRICE}, \texttt{DISCOUNT}, and \texttt{TAX}. For each column, the number of milliseconds used to create this lookup is measured.

\paragraph{Total Memory Footprint}
\label{par:Total Memory Footprint}
We measure the total memory footprint of the TPC-H Q1 \gd~application by using the Windows 10 memory manager such that all overhead in the .NET and user interface layers is included. The \textit{In Use} memory metric is recorded before the \gd~instance is loaded and when the UI is shown to the user. Since the measurement include more than our application, and are affected by background tasks and other running program, the total memory footprint output is not very accurate. However, it serves as a confirmation on other memory measurements, as well as an indication that total application memory consumption is reduced.

\section{Write Benchmark}
\label{bm:write}
\afigure{img/lineitem.png}{\texttt{LINEITEM} table layout. Courtesy of \cite{Transaction_Processing_Performance_Council_TPC2014-ux}.}{fig:lineitem}{0.8}
Since write performance might be sacrificed when optimizing for read performance, we establish a benchmark that measures this impact. For simplicity reasons, the benchmark is based on the \texttt{LINEITEM} table in the TPC-H benchmark. The main reason for using this table was availability of  different data types, which we see in Figure \ref{fig:lineitem}.

The benchmark reads $N$ items into a data source, for all items, the following fields are modified:
\begin{itemize}
    \item \texttt{QUANTITY = QUANTITY + 1} to measure integer performance.
    \item \texttt{EXTENDEDPRICE = EXTENDEDPRICE + 1.0} to measure floating point performance.
    \item \texttt{COMMENT = COMMENT + "1"} to measure string performance.
    \item \texttt{SHIPDATE = SHIPDATE PLUS 1 DAYS} to measure date performance.
\end{itemize}
\afigure{img/write-test-definition.png}{The task definition for the Write Benchmark. There is one loop per attribute.}{fig:write-test-definition}{0.4}
One property is modified per iteration, so there is a total of four iterations in this task. The full task definition is seen in Figure \ref{fig:write-test-definition}.

\subsection{Test Input}
\label{sub:Test Input}
The number of \texttt{LINEITEM}s loaded into the data source and modified. The number is specified before the task is run.

\subsection{Test Output}
\label{sub:Test Output}
For each property modified, the time it took to enumerate the entire data source is measured. In addition to this, the average time per modification is listed.

\section{Filter Benchmark}
\label{bm:filter}
\begin{table}
    \centering
    \begin{tabular}{c | c}
        Predicate & Selects (of 100,000) \\
        \hline
        \hline
        \texttt{LINESTATUS = F} & 49,864 \\
        \texttt{TAX >= 0.02} & 66,851 \\
        \texttt{EXTENDEDPRICE >= 1000} & 99,956 \\
        \texttt{EXTENDEDPRICE >= 40000} & 45,408\\
        \texttt{EXTENDEDPRICE < 1000} & 44 \\
        \texttt{EXTENDEDPRICE = 42995.94} & 1 \\
        \texttt{SHIPDATE IN 1995} & 15,435 \\
        \texttt{SHIPINSTRUCT\_NULL HAS VALUE} & 74,991 \\
    \end{tabular}
    \caption{Predicates Filter Benchmark. The second column indicated selectivity by showing the number of selected rows if an input size of 100,000 is used.}
    \label{tab:Filter Test}
\end{table}
To test \gap's ability to move data from one data source to another based on a filter, we create a benchmark based on the \texttt{LINEITEM} table from the TPC-H benchmark. This table is used for the same reason as for Benchmark \ref{bm:write}; there are several different data types available. 

The benchmark is composed by a list of predicates, where every object that satisfies the predicate are moved from the original data source to the filtered data source. The predicates are selected to span a variety of data types, selectivities, and operators. Since there are no null-columns in \texttt{LINEITEM}, a new column \texttt{SHIPINSTRUCT\_NULL} was created where all \texttt{NONE} values were set to \texttt{null}.

\subsection{Test Input}
\label{sub:Test Input}
The number of \texttt{LINEITEM}s loaded into the original data source.

\subsection{Test Output}
\label{sub:Test Output}
The timing result for the filter operation for each predicate; the time it takes to move the objects from the original data source to the filtered data source. Reproted in milliseconds.

\section{Real Customer Data Benchmark I: Data Mart Load}
\label{bm:odin-load}
The same benchmark as \ref{bm:q1} is run on real customer data.

\section{Real Customer Data Benchmark II: Create, Update, and Delete}
\label{bm:odin-crud}



%\section{OLTP Test}
%\label{sec:OLTP Test}
%Testing creating, updates, deletes, and everything.
%
%\subsection{Test Input}
%\label{sub:Test Input}
%
%\subsection{Test Output}
%\label{sub:Test Output}
%The time it takes to blabla bla
%
%
%\section{Real Customer Test: Shareholder Analysis}
%\label{sec:Real Customer Test: Shareholder Analysis}
%We also run some tests on some real test cases proposed by \genus. This motivated the entire research.
%
%Both test cases will be run and presented to the respective customers.
%
%Due to confidential information, the names and the specific product are not available in this report, and the respective companies are held anonymous.
%
%The first test case tests memory usage and load time for an analysis on stock ownership.
%\begin{itemize}
%    \item Distributor and Customer Reporting
%    \item Operations, Settlements and Fund Management Support
%    \item Payments and Payment matching
%    \item Commissions Management
%    \item Customer Relationship Management (CRM)
%    \item Portfolio and Company Management
%    \item Order processing
%\end{itemize}


%\subsection{Real Customer Data Test Case II: A Retail Chain}
%\label{sub:Real Customer Data Test Case II: A Retail Chain}
%\begin{itemize}
%    \item Manage operations for 2200 grocery stores
%    \item Corporate Performance Management (CPM)
%    \item Apps for Operational Control \& Monitoring
%    \item Employee- and customer surveys
%    \item Food Safety and Risk Management
%\end{itemize}
%Like above, this is tested for memory usage and load time.



