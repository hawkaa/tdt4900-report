\chapter{Background Theory}
\label{chap:background}
This chapter introduce important background theory relevant to this research. First, we introduce \mde. Then, to introduce a use-case where handling and analyzis of large datasets is required, we give a brief introduction to \bi~and \bd. Last, we study \delphi, the programming language used in \gap~core.

\todo{Is this a good pitch on why we introduce bi and bd?}
\clearpage


\section{Model-Driven Engineering}
\label{sec:Model-Driven Engineering}
Since the introduction of computers, researchers have been working to raise the abstraction level at which software developers create programs \cite{Atkinson2003-wr}. One example of this is compilers and how they programmers no longer need to know assembly language for the different processor architectures. Now, object-oriented programming languages enable software developers to create own abstractions and solve more complex problems than ever. \mde~is a continuation of this trend, where the goal is to automate many of the complex, but routine, programming tasks. Such tasks include interoperability, distribution, and persistence. One of the key technological foundations of \mde~is support for visual modeling.

\mde~research has identified that there is a gap between the problem domain and the software implementation domain \cite{France2007-ae}. Thus, most research focuses on bridging this gap. This process requires systematic experimentation and accumulation of experience, and new techniques should strive to reduce accidental complexities of the past.

The ability to express models with concepts that are closer to the problem domain and less coupled to the underlying implementation is one of the main advantages with \mde~compared to most popular programming languages \cite{Selic2003-qa}. This makes programs easier to understand and maintain, as well as making program specification and requirements easier to communicate.  In addition, according to Atkinson \ea, \mde has two more major advantages \cite{Atkinson2003-wr}:
\begin{itemize}
    \item In the short term, \mde~increase developer productivity and increase the amount of functionality an IT artifact can deliver.
    \item In the long term, \mdd~supporting infrastructures are more maintainable, and it takes a longer time before the artifact becomes obsolete.
\end{itemize}

A common discussion topic in the field of \mde~is the efficiency of the modeled programs. Criticism has pointed out that machines cannot optimize code better than a creative human being with clever tricks. However, it is widely known that modern compilers outperform most system developers, and it does so more reliably. The same observation can be done for most \mdd~supporting infrastructures. Current tools generate programs that are within 5 to 15 percent effective as hand-crafted systems, both regarding memory consumption and performance. However, there are still critical cases where manually written code significantly outperforms \mde, which has often been used as an excuse to discard an \mdd~approach altogether.

\subsection{Model Levels}
\label{sub:Model Levels}
\afigure{img/uml-mof.png}{Traditional \mdd~with Unified Modeling Language (UML) and Meta-Object Factory (MOF) model levels. (Adapted from \cite{Atkinson2003-wr})}{fig:uml-mof}{0.7}
Central to \mde~is the use of models. According to Leppänen M, a model is a thing that is used to help the understanding of some other things \cite{Leppanen2006-ay}. In \mdd, multiple levels of models are defined. In Leppänen's framework there are three different levels of models: The instance model, the type model, and the meta model. A meta model is something used to help the understanding and communication analysis and design of type models. A model on a higher level describes/ prescirbes models on the next lower level. A good model must appeal to our intuition, and it provides a shortuct by reducing the amount of intellectual effort required to understand \cite{Selic2013-qa}.

% Ønsker å bruke disse nivåene, kanskje med eksempler fra Bergheim.
They also correspond to the information system layer, information system development layer, the method engineering layer, and research work layer.

Although there are many ways to define these levels, we list the levels defined by Bergheim et al.
\begin{itemize}
    \item Operational level - Changes of state in an application
    \item Next meta level - Descriptions of a specific application
    \item B-level - how to make programming tool, like the java programming language itself
    \item Higest level is to make intances on feth B level, how to make different formalisms.
\end{itemize}

The first generation of \mdd~infrastructure is illustrated in Figure \ref{fig:uml-mof} and is Unified Modeling Language (UML) and Meta Object Facility (MOF) \cite{Atkinson2003-wr}. However, although this approach has served well in defining some central components and terminology in \mde, it has been critisized for lacking a description of how the different layers relate to the real world. It also has a single-instace of relationship to define the levels, which causes inconsistencies.

\afigure{img/linguistic-metamodeling.png}{Linguistic metamodeling view. (Adapted from \cite{Atkinson2003-wr})}{fig:linguistic-metamodeling}{0.7}
To overcome the challenges with UML, linguistic metamodeling to the rescue. This separates linguistic relationships from ontological ones. This is seen in Figure \ref{fig:linguistic-metamodeling}.

\subsection{Model-Driven Engineering in Practice}
\label{sub:Model-Driven Engineering in Practice}
\mendix, one of the market players in \mdd, uses a JavaScript blabla.

\gap~is another \mdd~suporting infrastructure, and is studied in Chapter \ref{chap:gap}.

\section{Business Intelligence}
\label{sec:Business Intelligence}
\bi~is normally described as tools and techniques used to transform unstructured data into useful and meaningful information that can be used to support decisions \cite{Wikipedia_contributors2015-ag}. The goal is to allow for easy interpretation and gain insight in the data, such that businesses end up with a competitive market advantage. \bi~software can assist in making a wide range of business decisions, including strategic decisions as goals and product pricing or positioning, as well as operational decisions like priorities.

A challenge in \bi~can be information overflow. To know which information is needed in a \bi~application, decision makers must be aware of which types of decisions they should make, and have a model for each \cite{Ackoff1999-wk}. Since managers rarely fulfil the latter requirement, they add a safety factor and asks the IT department to provide everything. The result is information overload, and the much of the data is irrelevant. 

We introduce \bi~in this thesis to explain a common use-case where the ability to handle and analyze large datasets is required. \bi~reqirements in \gap~is one of the key motivations to this research. \todo{Should this pitch be used?}

\subsection{Data Warehouses}
\label{sub:Data Warehouses}
Companies' need for \bi~has traditionally been solved by data warehouses. These data warehouses extract, transform, and load data into structures that are suited for analytical queries and report generation.

The challenges with data warehouses are many, among them, the lack of flexibility. Reports are usually preconfigured and implemented by the IT department, which can result in lengthy reporting backlogs. Besides, the \qlikview~developers have pointed out some other drawbacks of traditional query-based \bi~tools \cite{Qlik2010-ya}:
\begin{itemize}
  \item Only small subsets of the main dataset are extracted at a time. These subsets are divorced from the data that was not included in the query.
  \item Each query represents a single piece of information, and the information gathered from individual queries are hard to combine.
  \item Traditional systems do not maintain relationships between queries. A query can be hard to formulate, and it is not always easy to know what to look for. Traditional \bi~applications do not let the user build queries step by step.
\end{itemize}

\subsection{Star and Snowflake Schema}
\label{sub:Star and Snowflake Schema}
Many \bi~applications and data warehouses usually access data that is organized in a \textit{star or snowflake schema} \cite{Barber2012-xt}. Distinct for such schemas is that they have a huge fact table, which can have millions or billions of rows, and smaller dimension tables, each representing some aspect of the fact rows (e.g. category, region, time). The fact table is connected to the dimension tables using foreign keys. A snowflake schema is an extension of the star schema, where one or more dimension tables can have relationships that further describe a dimension.

Star and snowflake schemas are normally used in \bi~applications because they are easier to optimize \cite{Lamb2012-kg}. The query optimizer creates efficient query plans by filtering and applying joins on the most highly selective dimensions first. Secondly, queries on star and snowflake schemas are easier to anticipate, such that indexes, materialized views, and/or denormalization can be applied to improve query efficiency \cite{Barber2012-xt}.

The disadvantage of using star and snowflake schemas for \bi~is the lack of flexibility. There are certain situations where there are more than one large fact table and situations where there is no clear distinction between fact and dimension tables.

\section{Business Discovery}
\label{sec:Business Discovery}

To overcome the challenges with traditional \bi~systems, a new type of products have emerged. We call these for \bd~products, a notion that was introduced by \textit{Qlik}~\cite{Qlik2014-vd}. \bd~products normally build on in-memory technologies and are fast, elegant, and end user intuitive solutions to analyse business data. Examples of such products are \powerpivot, \tableau, and \qlikview. We have only studied \qlikview~and \tableau~in this thesis.

\ffigure{img/qlik-hierarchy.png}{Comparison of a traditional reporting application and \qlikview. Traditional \bi~applications normally have predefined drill-down paths. \qlikview~allows the user decide where to start and end. (Adapted from \cite{Qlik2014-vd})}{fig:qlik-hierarchy}

\bd~products allow users to follow their "information scent" or "train of thought" when navigating through the data \cite{Kamkolkar2015-iq, Qlik2014-vd}. As seen in Figure \ref{fig:qlik-hierarchy}, there are no prespecified drill-down patterns, and users decide where to start and end. In these applicaitons, grouping, joining, and calculations are performed on-the-fly. High-performance, in-memory technologies are used to enable such functionality. 

In the next sections, we explain how a typical \bd~application works by using \qlikview~primarily as an example. \tableau~and \powerpivot~work similarly.

\subsection{Data Import}
\label{sub:Data Import}
Data is loaded into a \qlikview~document using a data import script that connects the application with data sources like databases or files. The script uses an SQL-like syntax that lets the user specify names of fields and tables that are used in the data analysis. When the data import script is run, data is fetched from the sources and put into the \qlikview~in-memory engine such that data can be queried efficiently. No queries are proxied to the underlying data sources, \qlikview~manage all queries internally.

\afigure{img/qlikview-association.png}{Four tables: Countries, customers, transactions, and memberships. The fields \textit{Country} and \textit{CustomerID} associate the tables. (Adapted from \cite{QlikTech_International_AB2011-ov})}{fig:qlikview-association}{0.7}

The data import script will regularly include more than one table. Multiple tables are \textit{associated} if they have fields with the same name, as seen in Figure \ref{fig:qlikview-association}. Internally, there can be only one data connector between a pair of tables, so if multiple tables refer to a single table, a mechanism (either \textit{synthetic keys} or \textit{loose coupling}) must be applied to logically duplicate the table. This restriction ensures that there exists no more than one possible join path between any pairs of tables \cite{noauthor_undated-js}.

In the data import step, data might be preprocessed and transformed. First, data can be aggregated, for instance by summarizing or grouping, before being loaded into \qlikview. Second, data might be filtered. Both these techniques reduce data volumes. Lastly, data can be denormalized in the import script, i.e. pre-joining tables before import. Denormalizing can be done to lower the number of tables in the data extract and, by doing so, reduce application complexity.

\subsection{User Interface}
\label{sub:User Interface}
\ffigure{img/qlik-panel.png}{\qlikview~dashboard with various GUI elements, like lists and charts. Selections are green, matched data is white, and unrelated data is grey. (Adapted from \cite{Qlik2014-vd})}{fig:qlik-panel}
Users interact with the \bd~application through a reporting dashboard, as seen in Figure~\ref{fig:qlik-panel}. Requirements for such panel are typically \cite{Qlik2014-vd}:
\begin{itemize}
  \item Clicking field values in list boxes.
  \item Lassoing data in charts, graphs, and maps.
  \item Manipulating sliders.
  \item Choosing dates in calendars.
  \item Cycling through different chart types.
\end{itemize}

Users navigate through the data by making selections in the user interface. When a selection is made, the item is made green, as seen in Figure \ref{fig:qlik-panel}. The current selection is also known as the \term{application state}. Upon selection, the rest of the elements in the panel are updated based on the new application state; aggregations are recalculated, and graphs and lists are updated. \qlikview~colors matched elements white and unmatched elements gray. Dashboards are typically available from different devices, including desktop comptures, tablets, and mobile phones.

\subsection{Business Discovery and Queries}
\label{sub:Business Discovery and QueriesL}

We see that \bd~products interact with the data using selections and filters in a reporting panel. This technique is different from database systems designed for OLAP workloads, which normally use a \textit{query-based} interface, usually SQL. Still, \bd~products need to provide most functionality that SQL databases do, like row listing, filtering, joining, grouping and aggregation, and sorting.

Queries, or requests, in a \bd~application have certain characteristics. First of all, they cannot be anticipated, more specifically, they are \textit{ad-hoc}. Secondly, there is a limit to the number of returned rows. Users are interested in queries that can be analyzed quickly, so we do not expect a \bd~application to return thousands of rows as a result from a single table \cite{Ferrari2012-hm}. Also, the user interface has limitations in how much data that can be displayed at the same time. 

\section{Delphi Programming Language}
\label{sec:Delphi Programming Language}
Since the system we will study in this thesis, the \gap, we need to study some background theory of how this language works. \delphi~is both a programming language and an integrated development environment \cite{Wikipedia_contributors2016-jk}, but we will only consider the programming language in this section. The language is an \name{Object Pascal} dialect and compiles into native code for several platforms, including \name{OSX} 32-bit architecture and \name{Windows} 32- and 64-bit architectures. It has a more efficient compiler than most languages. Delphi is classified as a strongly typed high-level language. Delphi supports object oriented design with polymorphism and interfaces.

Delphi support strings with reference counting \cite{Wikipedia_contributors2016-jk}, and the memory for such is managed without programmer intervention. It supports unicode strings.

In 2009, generics and full unicode support was introduced.

A program in \delphi~is composed of different units, with two mandatory parts: The interface, much like a header in C, that declares constants, types, variables, and functions that are available from the outside, and the implementation, that contains the actual code \cite{noauthor_undated-pl}. Types, variables, etc. that are declared in the implentation part only are inaccesible outside the unit. In addition, each unit might specify an initialization and finalization code chunk to run on unit load and unit unload respectively. One unit may depend on other units by specifying the dependencies in the \texttt{uses}. Hence, the dependency graph is automatically created and no makefiles are needed. Cricular dependencies cause compilation error, and are not resolved automatically \cite{noauthor_undated-sp}.

It supports preprocessing directives, but not macros.

\delphi~has operators for referencing and dereferencing values, and supports basic pointer arithmetic \cite{noauthor_undated-cn}.

\subsection{Delphi Types}
\label{sub:Delphi Types}

Simple types can be divided into two categories: Ordinal and real \cite{noauthor_undated-st}. Integer types is one of the ordinal value types, and have both platform-dependend and platform-independent implementations. Platform-dependent is always encouraged due to performance benefits.

\subsubsection{String Types}
\label{ssub:String Types}
\afigure{img/delphi-strings.png}{Strings in \delphi}{fig:delphi-strings}{0.8}
There are two major character implementations, \cn{AnsiChar} and \cn{WideChar}, with 1 or 2 bytes each respectively. The latter is default.

There is support for both single and double precision floating point numbers.

There are both Ansi and Unicode strings, where the default is Unicode \cite{noauthor_undated-cp}. There is support for both UTF-8 and UTF-16 encodings. The latter is default. Both strings are dynamically allocated, and is limited by the available memory. They are allocated on the heap. A string variable is a pointer with reference counting, length, data length and code page. The compiler tries to exploit the nature of the reference counting, and it has copy-on-write semantics if one reference changes the string. Unicode strings defaults to UTF-16. These string types are not null-terminated. Unicode strings are also managed automatically by the compiler.

\subsubsection{Structured Types}
\label{ssub:Structured Types}
Commonly used in \delphi. A structured type can be a set, array, record, file, class, and more \cite{noauthor_undated-vu}. A structure is by default byte-aligned.

One such structured type, is the array. An array can be static or dynamic. The static arrays are allocated with a certain length, and will take all the space, even if values are not assigned to all indexes. The dynamic arrays are reallocated when you assign a value that is not yet been assigned or you call the \fn{SetLength} method on them. The arrays are ref-counted the same way as a string, but they are not copy-on-write.

Another type, the \cn{record} type, commonly represents a heterogenous set of values. A record might also have a variant part, which is not allowed in classes. Records are allocated on the stack by default and has value semantics, like copy on assignment, passed by value etc.

\subsubsection{Pointer Types}
\label{ssub:Pointer Types}
A pointer is a 64-bit value on a 64-bit architecture. Pointers are typed to indicate which type they hold, much like C and C++.

Both function and method pointers are supported.

Delphi supports variant types for types that cannot be determined at compile time \cite{noauthor_undated-mx}. Althogh variants are powerful, they consume more memory and are slower than statically bound data types, and errors that earlier were caught compile time, will now happen at run-time.

Variables are allocated on the stack by default, but can also be dynamically created on the heap \cite{noauthor_undated-lw}.

\subsection{Memory Management}
\label{sub:Memory Management}
The memory manager is different for different platforms \cite{noauthor_undated-ys}. On Windows, which is the platform used by \gap, the \name{FastMM} memory manager is used. This memory manager is optimized for programs that allocate small to medium size blocks. Block size are rounded up to the nearest 16 bytes. It anticipates future block reallocations which reduces address space fragmentation.

Arrays are also ref-counted.

Classes are 64-bit pointers. Variables are held in the same order which they are declared on the stack. On a 64-bit platform, the first 8 bytes are pointers to a virtual method table.

\subsection{Functions and Methods}
\label{sub:Functions and Methods}
The default calling convention is \texttt{register}, which uses three registers and puts the rest on the stack \cite{noauthor_undated-xi}. This is most efficient, because it does not need to create a stack frame on every function invocation. In addition, all properties contain either none, one, or two parameters, hence are quite efficient. 

Function overloading is supported.

\subsection{Inlining}
\label{sub:Inlining}
The compiler supports inlining of methods using the \texttt{inline} directive. However, this does not work for virtual methods, across package boundaries \cite{noauthor_undated-rx} \todo{finne mer her}

\subsection{Program Control}
\label{sub:Program Control}
Value and constant parameters to a method call is either passed by reference or by value, depending on the size of the variable \cite{noauthor_undated-ae}. By default, the \fn{register} calling convention is used, which means up to three parameters can be stored in registers. Some types, including doubles and 64-bit integers, are always pushed on the stack.

\subsection{Misc}
\label{sec:Misc}
\delphi~is object oriented, and supports interfaces, inheritance, polymorphism and generics. In addition, inline assembly is supported, where \delphi~identifiers are allowed \cite{noauthor_undated-px}.

There is support for Generics, where things can be parameterized by type \cite{noauthor_undated-sx}. Used to decouple an algorithm from its data structure. On an instantiation, it results in a code duplication.

\subsection{Standard library}
\label{sub:Standard library}
Some built-in classes in the standard library include:
\begin{itemize}
    \item \cn{TBits} is a built in type that represents a bitmap. It works like an array of booleans, except that it is optimized for speed using assemply code. In addition, it contains an utility method \fn{OpenBit}, which finds the first 0, or the open bit, in the bitmap.
    \item \cn{TArray} is the generic implementation of \cn{array of *}. It is ref-counted, dynamically allocated using the \texttt{SetLength} function. Its performance is studied in Appendix \ref{app:array-performance}.
    \item \cn{TList} and \cn{TObjectList} are wrappers for \cn{TArray} that automatically allocates enough space. \cn{TObjectList} is like \cn{TList}, but it assumes ownership over objects and automatically frees objects that are removed from the list.
\end{itemize}

