
@MISC{noauthor_undated-fi,
  title        = "{Self-Service} {BI} (Business Intelligence) | Information
                  Builders",
  abstract     = "With self-service BI (Business Intelligence), non-technical
                  professionals can generate their own reports, run their own
                  queries, and conduct their own analyses, without the
                  assistance of IT staff.",
  howpublished = "\url{http://www.informationbuilders.com/self-service-bi}",
  note         = "Accessed: 2015-12-7"
}

@ARTICLE{Devlin1988-yu,
  title    = "An architecture for a business and information system",
  author   = "Devlin, B A and Murphy, P T",
  abstract = "The transaction-processing environment in which companies
              maintain their operational databases was the original target for
              computerization and is now well understood. On the other hand,
              access to company information on a large scale by an end user for
              reporting and data analysis is relatively new. Within IBM, the
              computerization of informational systems is progressing, driven
              by business needs and by the availability of improved tools for
              accessing the company data. It is now apparent that an
              architecture is needed to draw together the various strands of
              informational system activity within the company. IBM Europe,
              Middle East, and Africa (E/ME/A) has adopted an architecture
              called the E/ME/A Business Information System (EBIS) architecture
              as the strategic direction for informational systems. EBIS
              proposes an integrated warehouse of company data based firmly in
              the relational database environment. End-user access to this
              warehouse is simplified by a consistent set of tools provided by
              an end-user interface and supported by a business data directory
              that describes the information available in user terms. This
              paper describes the background and components of the architecture
              of EBIS.",
  journal  = "IBM Syst. J.",
  volume   =  27,
  number   =  1,
  pages    = "60--80",
  year     =  1988
}

@MISC{Wikipedia_contributors2015-zu,
  title        = "Multiway data analysis",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "Multiway data analysis is a method of analyzing large data
                  sets by representing the data as a multidimensional array.
                  The proper choice of array dimensions and analysis techniques
                  can reveal patterns in the underlying data undetected by
                  other methods.[1]",
  month        =  "5~" # nov,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Multiway_data_analysis&oldid=689137876}",
  note         = "Accessed: 2015-12-7"
}

@MANUAL{Transaction_Processing_Performance_Council_TPC2014-ux,
  title  = "Transaction Processing Performance Council",
  author = "{Transaction Processing Performance Council (TPC)}",
  month  =  "13~" # nov,
  year   =  2014
}

@MANUAL{QlikTech_International_AB2011-ov,
  title  = "Reference Manual",
  author = "{QlikTech International AB}",
  month  =  apr,
  year   =  2011
}

@MISC{Wikipedia_contributors2015-rk,
  title        = "Short-circuit evaluation",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "Short-circuit evaluation, minimal evaluation, or McCarthy
                  evaluation denotes the semantics of some Boolean operators in
                  some programming languages in which the second argument is
                  executed or evaluated only if the first argument does not
                  suffice to determine the value of the expression: when the
                  first argument of the AND function evaluates to false, the
                  overall value must be false; and when the first argument of
                  the OR function evaluates to true, the overall value must be
                  true. In some programming languages (Lisp), the usual Boolean
                  operators are short-circuit. In others (Java, Ada), both
                  short-circuit and standard Boolean operators are available.
                  For some Boolean operations, like XOR, it is not possible to
                  short-circuit, because both operands are always required to
                  determine the result.",
  month        =  "21~" # nov,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Short-circuit_evaluation&oldid=691686570}",
  note         = "Accessed: 2015-12-4"
}

@MISC{Wikipedia_contributors2015-od,
  title        = "Call stack",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "In computer science, a call stack is a stack data structure
                  that stores information about the active subroutines of a
                  computer program. This kind of stack is also known as an
                  execution stack, control stack, run-time stack, or machine
                  stack, and is often shortened to just ``the stack''. Although
                  maintenance of the call stack is important for the proper
                  functioning of most software, the details are normally hidden
                  and automatic in high-level programming languages. Many
                  computer instruction sets provide special instructions for
                  manipulating stacks.",
  month        =  "1~" # oct,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Call_stack&oldid=683600635}",
  note         = "Accessed: 2015-12-4"
}

@MISC{Wikipedia_contributors2015-zc,
  title        = "Loop unrolling",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "Loop unrolling, also known as loop unwinding, is a loop
                  transformation technique that attempts to optimize a
                  program's execution speed at the expense of its binary size,
                  which is an approach known as the space-time tradeoff. The
                  transformation can be undertaken manually by the programmer
                  or by an optimizing compiler.",
  month        =  "29~" # aug,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Loop_unrolling&oldid=678495551}",
  note         = "Accessed: 2015-12-4"
}

@MISC{Wikipedia_contributors2015-lw,
  title        = "Scalability",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "Scalability is the capability of a system, network, or
                  process to handle a growing amount of work, or its potential
                  to be enlarged in order to accommodate that growth.[1] For
                  example, it can refer to the capability of a system to
                  increase its total output under an increased load when
                  resources (typically hardware) are added. An analogous
                  meaning is implied when the word is used in an economic
                  context, where scalability of a company implies that the
                  underlying business model offers the potential for economic
                  growth within the company.",
  month        =  "13~" # nov,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Scalability&oldid=690445208}",
  note         = "Accessed: 2015-12-3"
}

@MISC{CloudBoost2015-gg,
  title        = "{CloudBoost.io} | {NoSQL} Database as a Service - Storage,
                  Search, Real-time with one {API}",
  booktitle    = "{CloudBoost}",
  author       = "{CloudBoost}",
  abstract     = "CloudBoost is one complete Database Solution for your apps
                  which does data-storage, search and realtime, graphs,
                  time-series and much more.",
  month        =  "16~" # jun,
  year         =  2015,
  howpublished = "\url{https://www.cloudboost.io}",
  note         = "Accessed: 2015-12-3"
}

@MISC{Wikipedia_contributors2015-kp,
  title        = "Superscalar processor",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "A superscalar processor is a CPU that implements a form of
                  parallelism called instruction-level parallelism within a
                  single processor. It therefore allows faster CPU throughput
                  (the number of instructions that can be executed in a unit of
                  time) than would otherwise be possible at a given clock rate.
                  A superscalar processor executes more than one instruction
                  during a clock cycle by simultaneously dispatching multiple
                  instructions to different execution units on the processor.
                  Each execution unit is not a separate processing unit (called
                  ``core'') as in multi-core processors, but an execution
                  resource within a single CPU such as an arithmetic logic
                  unit, a bit shifter, or a multiplier.",
  month        =  "27~" # oct,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Superscalar_processor&oldid=687770458}",
  note         = "Accessed: 2015-12-3"
}

@BOOK{Ackoff1999-wk,
  title     = "Ackoff's best: his classic writings on management",
  author    = "Ackoff, Russell Lincoln",
  publisher = "John Wiley \& Sons",
  year      =  1999
}

@MISC{Wikipedia_contributors2015-pb,
  title        = "Database",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "A database is an organized collection of data.[1] It is the
                  collection of schemas, tables, queries, reports, views and
                  other objects. The data is typically organized to model
                  aspects of reality in a way that supports processes requiring
                  information, such as modelling the availability of rooms in
                  hotels in a way that supports finding a hotel with vacancies.",
  month        =  "2~" # dec,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Database&oldid=693373678}",
  note         = "Accessed: 2015-12-3"
}

@MISC{Wikipedia_contributors2015-cw,
  title        = "Online transaction processing",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "Online transaction processing, or OLTP, is a class of
                  information systems that facilitate and manage
                  transaction-oriented applications, typically for data entry
                  and retrieval transaction processing. The term is somewhat
                  ambiguous; some understand a ``transaction'' in the context
                  of computer or database transactions, while others (such as
                  the Transaction Processing Performance Council) define it in
                  terms of business or commercial transactions.[1] OLTP has
                  also been used to refer to processing in which the system
                  responds immediately to user requests. An automated teller
                  machine (ATM) for a bank is an example of a commercial
                  transaction processing application. Online transaction
                  processing applications are high throughput and insert or
                  update-intensive in database management. These applications
                  are used concurrently by hundreds of users. The key goals of
                  OLTP applications are availability, speed, concurrency and
                  recoverability.[2] Reduced paper trails and the faster, more
                  accurate forecast for revenues and expenses are both examples
                  of how OLTP makes things simpler for businesses. However,
                  like many modern online information technology solutions,
                  some systems require offline maintenance, which further
                  affects the cost--benefit analysis of online transaction
                  processing system.",
  month        =  "22~" # nov,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Online_transaction_processing&oldid=691859924}",
  note         = "Accessed: 2015-12-3"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Wikipedia_contributors2015-hw,
  title        = "Online analytical processing",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "In computing, online analytical processing, or OLAP
                  (/ˈoʊl\ae{}p/), is an approach to answering multi-dimensional
                  analytical (MDA) queries swiftly.[1] OLAP is part of the
                  broader category of business intelligence, which also
                  encompasses relational database, report writing and data
                  mining.[2] Typical applications of OLAP include business
                  reporting for sales, marketing, management reporting,
                  business process management (BPM),[3] budgeting and
                  forecasting, financial reporting and similar areas, with new
                  applications coming up, such as agriculture.[4] The term OLAP
                  was created as a slight modification of the traditional
                  database term online transaction processing (OLTP).[5]",
  month        =  "3~" # dec,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Online_analytical_processing&oldid=693548894}",
  note         = "Accessed: 2015-12-3"
}

@MISC{Wikipedia_contributors2015-ax,
  title        = "{SIMD}",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "Single instruction, multiple data (SIMD), is a class of
                  parallel computers in Flynn's taxonomy. It describes
                  computers with multiple processing elements that perform the
                  same operation on multiple data points simultaneously. Thus,
                  such machines exploit data level parallelism, but not
                  concurrency: there are simultaneous (parallel) computations,
                  but only a single process (instruction) at a given moment.
                  SIMD is particularly applicable to common tasks like
                  adjusting the contrast in a digital image or adjusting the
                  volume of digital audio. Most modern CPU designs include SIMD
                  instructions in order to improve the performance of
                  multimedia use.",
  month        =  "14~" # nov,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=SIMD&oldid=690570287}",
  note         = "Accessed: 2015-12-2"
}

@MISC{Wikipedia_contributors2015-vg,
  title        = "Instruction-level parallelism",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "Instruction-level parallelism (ILP) is a measure of how many
                  of the operations in a computer program can be performed
                  simultaneously. The potential overlap among instructions is
                  called instruction level parallelism.",
  month        =  "4~" # nov,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Instruction-level_parallelism&oldid=689101114}",
  note         = "Accessed: 2015-12-2"
}

@MISC{Wikipedia_contributors2015-yx,
  title        = "Hyper-threading",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "Hyper-threading (officially called Hyper-Threading Technology
                  or HT Technology, and abbreviated as HTT or HT) is Intel's
                  proprietary simultaneous multithreading (SMT) implementation
                  used to improve parallelization of computations (doing
                  multiple tasks at once) performed on x86 microprocessors. It
                  first appeared in February 2002 on Xeon server processors and
                  in November 2002 on Pentium 4 desktop CPUs.[4] Later, Intel
                  included this technology in Itanium, Atom, and Core 'i'
                  Series CPUs, among others.",
  month        =  "9~" # nov,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Hyper-threading&oldid=689798520}",
  note         = "Accessed: 2015-12-2"
}

@MISC{noauthor_undated-bk,
  title        = "The End of a Necessary Evil: Collapsing the Memory Hierarchy",
  booktitle    = "{HP} Next",
  abstract     = "Martin Fink explores the reasons why computers spend most of
                  their time shuffling data between different storage types and
                  what HP Labs is doing about it",
  howpublished = "\url{http://www8.hp.com/hpnext/posts/end-necessary-evil-collapsing-memory-hierarchy#.Vl3rfSCrT0o}",
  note         = "Accessed: 2015-12-1"
}

@BOOK{Bratbergsengen2015-ed,
  title  = "Storing and Management of Large Data Volumes",
  author = "Bratbergsengen, Kjell",
  month  =  "27~" # mar,
  year   =  2015
}

@MISC{noauthor_undated-es,
  title        = "What is the relationship between dimensions, attributes, and
                  measures? - Quora",
  howpublished = "\url{https://www.quora.com/Data-Warehousing/What-is-the-relationship-between-dimensions-attributes-and-measures}",
  note         = "Accessed: 2015-11-30"
}

@MISC{noauthor_undated-lv,
  title        = "Genus",
  abstract     = "Genus AS is a Norwegian company and software house located in
                  Oslo. Genus\textregistered{} Application Framework is our
                  product, and used by our partners and customers to make
                  business software applications without programming.",
  howpublished = "\url{http://www.genus.no/?PageKey=91ff59b3-216e-4c89-8bd8-d9d366cc8479}",
  note         = "Accessed: 2015-11-29"
}

@MISC{noauthor_undated-jo,
  title        = "{TPC-H} - Homepage",
  abstract     = "The Transaction Processing Performance Council defines
                  transaction processing and database benchmarks and delivers
                  trusted results to the industry.",
  howpublished = "\url{http://www.tpc.org/tpch/}",
  note         = "Accessed: 2015-11-28"
}

@MISC{noauthor_undated-xi,
  title        = "Indexes",
  howpublished = "\url{https://docs.oracle.com/cd/B10500_01/server.920/a96520/indexes.htm}",
  note         = "Accessed: 2015-11-26"
}

@MISC{Wikipedia_contributors2015-lq,
  title        = "Bloom filter",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "A Bloom filter is a space-efficient probabilistic data
                  structure, conceived by Burton Howard Bloom in 1970, that is
                  used to test whether an element is a member of a set. False
                  positive matches are possible, but false negatives are not,
                  thus a Bloom filter has a 100\% recall rate. In other words,
                  a query returns either ``possibly in set'' or ``definitely
                  not in set''. Elements can be added to the set, but not
                  removed (though this can be addressed with a ``counting''
                  filter). The more elements that are added to the set, the
                  larger the probability of false positives.",
  month        =  "1~" # nov,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Bloom_filter&oldid=688490531}",
  note         = "Accessed: 2015-11-26"
}

@MISC{noauthor_undated-hp,
  title        = "Indexes",
  howpublished = "\url{http://docs.oracle.com/cd/B28359_01/server.111/b28313/indexes.htm}",
  note         = "Accessed: 2015-11-26"
}

@MISC{Stoimen_undated-js,
  title        = "Computer Algorithms: Data Compression with Run-length
                  Encoding",
  author       = "{Stoimen}",
  abstract     = "Introduction No matter how fast today's computers and
                  networks are, the users will constantly need faster and
                  faster services. To reduce the volume of the",
  howpublished = "\url{http://www.stoimen.com/blog/2012/01/09/computer-algorithms-data-compression-with-run-length-encoding/}",
  note         = "Accessed: 2015-11-25"
}

@MISC{Victor_Lavrenko2014-hv,
  title     = "Indexing 6: delta encoding (compression)",
  author    = "{Victor Lavrenko}",
  abstract  = "Inverted lists can be efficiently compressed using a combination
               of delta encoding and v-byte compression. Delta encoding
               converts document ids and positions...",
  publisher = "Youtube",
  month     =  "20~" # jan,
  year      =  2014,
  keywords  = "tts; text; text technologies; retrieval; information retrieval;
               search; search engine; IR; indexing; search index; inverted
               index; inverted list; fast search; efficiency; search
               efficiency; data structure; Search Engine Indexing; Google;
               compression; index compression; delta encoding; delta coding"
}

@MISC{Wikipedia_contributors2015-cb,
  title        = "Delta encoding",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "Delta encoding is a way of storing or transmitting data in
                  the form of differences (deltas) between sequential data
                  rather than complete files; more generally this is known as
                  data differencing. Delta encoding is sometimes called delta
                  compression, particularly where archival histories of changes
                  are required (e.g., in revision control software).",
  month        =  "7~" # oct,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Delta_encoding&oldid=684583969}",
  note         = "Accessed: 2015-11-25"
}

@MISC{noauthor_undated-us,
  title        = "Understanding Tableau Data Extracts",
  booktitle    = "Tableau Software",
  abstract     = "This is the first post in a three-part series that will take
                  a large amount of information about Tableau data extracts,
                  highly compress that information, and place it into memory
                  --- yours.",
  howpublished = "\url{http://www.tableau.com/about/blog/2014/7/understanding-tableau-data-extracts-part1}",
  note         = "Accessed: 2015-11-25"
}

@MISC{noauthor_undated-vr,
  title        = "{TPC-H} - Top Ten Performance Results",
  abstract     = "The Transaction Processing Performance Council defines
                  transaction processing and database benchmarks and delivers
                  trusted results to the industry.",
  howpublished = "\url{http://www.tpc.org/tpch/results/tpch_perf_results.asp}",
  note         = "Accessed: 2015-11-24"
}

@MISC{Wikipedia_contributors2015-ag,
  title        = "Business intelligence",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "Business intelligence (BI) is often described as ``the set of
                  techniques and tools for the transformation of raw data into
                  meaningful and useful information for business analysis
                  purposes''[citation needed]. The term ``data surfacing'' is
                  also more often associated with BI functionality. BI
                  technologies are capable of handling large amounts of
                  unstructured data to help identify, develop and otherwise
                  create new strategic business opportunities. The goal of BI
                  is to allow for the easy interpretation of these large
                  volumes of data. Identifying new opportunities and
                  implementing an effective strategy based on insights can
                  provide businesses with a competitive market advantage and
                  long-term stability.[1]",
  month        =  "18~" # nov,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Business_intelligence&oldid=691203830}",
  note         = "Accessed: 2015-11-22"
}

@MISC{Wikipedia_contributors2015-az,
  title        = "Denormalization",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "In computing, denormalization is the process of attempting to
                  optimize the read performance of a database by adding
                  redundant data or by grouping data.[1][2] In some cases,
                  denormalization is a means of addressing performance or
                  scalability in relational database software.",
  month        =  "2~" # jun,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Denormalization&oldid=665191267}",
  note         = "Accessed: 2015-11-17"
}

@BOOK{Ang2014-nm,
  title     = "Research Design for Business \& Management",
  author    = "Ang, Siah Hwee",
  publisher = "Sage",
  year      =  2014
}

@BOOK{Oates2005-oi,
  title     = "Researching information systems and computing",
  author    = "Oates, Briony J",
  publisher = "Sage",
  year      =  2005
}

@INCOLLECTION{Muller2015-fr,
  title     = "Aggregates Caching in Columnar {In-Memory} Databases",
  booktitle = "In Memory Data Management and Analysis",
  author    = "M{\"{u}}ller, Stephan and Plattner, Hasso",
  publisher = "Springer International Publishing",
  pages     = "69--81",
  series    = "Lecture Notes in Computer Science",
  year      =  2015
}

@INPROCEEDINGS{Faust2012-or,
  title     = "Fast Lookups for {In-Memory} Column Stores: {Group-Key} Indices,
               Lookup and Maintenance",
  booktitle = "{ADMS@} {VLDB}",
  author    = "Faust, Martin and Schwalb, David and Krueger, Jens and Plattner,
               Hasso",
  pages     = "13--22",
  year      =  2012
}

@MISC{noauthor_undated-js,
  title        = "The underlying technology of {QlikView} | {DBMS} 2 :
                  {DataBase} Management System Services",
  booktitle    = "{DBMS2}",
  abstract     = "QlikTech* finally decided both to become a client and, surely
                  not coincidentally, to give me more technical detail about
                  QlikView than it had when last we",
  howpublished = "\url{http://www.dbms2.com/2010/06/12/the-underlying-technology-of-qlikview/}",
  note         = "Accessed: 2015-10-28"
}

@MISC{noauthor_undated-vq,
  title        = "Columnstore Indexes Described",
  booktitle    = "Microsoft Developer Network",
  howpublished = "\url{https://msdn.microsoft.com/en-us/library/gg492088.aspx}",
  note         = "Accessed: 2015-10-28"
}

@TECHREPORT{Exasol2014-xh,
  title  = "{EXASolution} Technical Whitepaper: A Peek under the Hood",
  author = "{EXASOL}",
  year   =  2014
}


@TECHREPORT{Kamkolkar2015-iq,
  title  = "Tableau for the Enterprise: An Overview for {IT}",
  author = "Kamkolkar, Neelesh and Fields, Ellie and Rueter, Marc",
  year   =  2015
}

@TECHREPORT{Qlik2013-an,
  title  = "Impact of {NUMA} on {QlikView} 11 Performance",
  author = "{Qlik}",
  month  =  sep,
  year   =  2013
}

@TECHREPORT{Qlik2012-ku,
  title  = "{QlikView} Server Linear Scaling",
  author = "{Qlik}",
  month  =  jun,
  year   =  2012
}

@TECHREPORT{Qlik2012-ku,
  title  = "Scaling Up vs. Scaling Out in a {QlikView} Environment",
  author = "{Qlik}",
  month  =  feb,
  year   =  2012
}

@TECHREPORT{Qlik2011-yc,
  title  = "{QlikView} Scalability Overview",
  author = "{Qlik}",
  month  =  apr,
  year   =  2011
}

@TECHREPORT{Qlik2011-ef,
  title  = "{QlikView} Architecture and System Resource Usage",
  author = "{Qlik}",
  month  =  apr,
  year   =  2011
}

@TECHREPORT{Qlik2011-hj,
  title  = "{QlikView} Security Overview",
  author = "{Qlik}",
  month  =  feb,
  year   =  2011
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@TECHREPORT{Qlik2010-ya,
  title  = "The Associative Experience: {QlikView’s} overwhelming advantage",
  author = "{Qlik}",
  month  =  oct,
  year   =  2010
}

@TECHREPORT{Bereanu2010-tj,
  title  = "The {SAP} Business Intelligence Complexity Conundrum",
  author = "Bereanu, Adrian and Espinosa, Lucia",
  year   =  2010
}

@TECHREPORT{Qlik2014-vd,
  title  = "What makes {QlikView} unique",
  author = "{Qlik}",
  month  =  jan,
  year   =  2014
}

@BOOK{Witten1999-qq,
  title     = "Managing gigabytes: compressing and indexing documents and
               images",
  author    = "Witten, Ian H and Moffat, Alistair and Bell, Timothy C",
  publisher = "Morgan Kaufmann",
  year      =  1999
}

@PHDTHESIS{Pavlic2002-nm,
  title  = "Operational Data Store",
  author = "Pavlic, Ivana",
  year   =  2002,
  school = "Norwegian University of Science and Technology"
}

@ARTICLE{Plattner2014-fr,
  title   = "The Impact of Columnar {In-Memory} Databases on Enterprise Systems",
  author  = "Plattner, Hasso",
  journal = "Proceedings VLDB Endowment",
  month   =  sep,
  year    =  2014
}

@INPROCEEDINGS{Faust2015-ke,
  title     = "Partitioned {Bit-Packed} Vectors for {In-Memory-Column-Stores}",
  booktitle = "Proceedings of the 3rd {VLDB} Workshop on {In-Memory} Data
               Mangement and Analytics",
  author    = "Faust, Martin and Flemming, Pedro and Schwalb, David and
               Plattner, Hasso",
  publisher = "ACM",
  pages     = "2",
  month     =  "31~" # aug,
  year      =  2015
}

@ARTICLE{Schwalb2014-hn,
  title   = "Efficient Transaction Processing for Hyrise in Mixed Workload
             Environments",
  author  = "Schwalb, David and Faust, Martin and Wust, Johannes and Grund,
             Martin and Plattner, Hasso",
  journal = "INTERNATIONAL WORKSHOP ON IN-MEMORY DATA MANAGEMENT AND ANALYTICS",
  year    =  2014
}

@MISC{noauthor_undated-wz,
  title        = "In Memory Data Management and Analysis - First and Second |
                  Arun Jagatheesan | Springer",
  abstract     = "This book constitutes the thoroughly refereed post conference
                  proceedings of the First and Second International Workshops
                  on In Memory Data Management and...",
  howpublished = "\url{http://www.springer.com/us/book/9783319139593}",
  note         = "Accessed: 2015-10-21"
}

@INPROCEEDINGS{Larson2011-kc,
  title     = "{SQL} Server Column Store Indexes",
  booktitle = "Proceedings of the 2011 {ACM} {SIGMOD} International Conference
               on Management of Data",
  author    = "Larson, Per-\AA{}ke and Clinciu, Cipri and Hanson, Eric N and
               Oks, Artem and Price, Susan L and Rangarajan, Srikumar and
               Surna, Aleksandras and Zhou, Qingqing",
  publisher = "ACM",
  pages     = "1177--1184",
  series    = "SIGMOD '11",
  year      =  2011,
  address   = "New York, NY, USA",
  keywords  = "column store, columnar index, data warehousing, olap"
}

@INPROCEEDINGS{Mehta1995-su,
  title     = "Managing intra-operator parallelism in parallel database systems",
  booktitle = "{VLDB}",
  author    = "Mehta, Manish and DeWitt, David J",
  volume    =  95,
  pages     = "382--394",
  year      =  1995
}

@INPROCEEDINGS{Kemper2011-ap,
  title       = "{HyPer}: A hybrid {OLTP\&OLAP} main memory database system
                 based on virtual memory snapshots",
  booktitle   = "Data Engineering ({ICDE)}, 2011 {IEEE} 27th International
                 Conference on",
  author      = "Kemper, Alfons and Neumann, Thomas",
  pages       = "195--206",
  institution = "IEEE",
  year        =  2011
}

@INPROCEEDINGS{Graefe1993-xn,
  title     = "The Volcano optimizer generator: extensibility and efficient
               search",
  booktitle = "Data Engineering, 1993. Proceedings. Ninth International
               Conference on",
  author    = "Graefe, G and McKenna, W J",
  abstract  = "The Volcano project, which provides efficient, extensible tools
               for query and request processing, particularly for
               object-oriented and scientific database systems, is reviewed. In
               particular, one of its tools, the optimizer generator, is
               discussed. The data model, logical algebra, physical algebra,
               and optimization rules are translated by the optimizer generator
               into optimizer source code. It is shown that, compared with the
               EXODUS optimizer generator prototype, the search engine of the
               Volcano optimizer generator is more extensible and powerful. It
               provides effective support for non-trivial cost models and for
               physical properties such as sorting order. At the same time, it
               is much more efficient, as it combines dynamic programming with
               goal-directed searching and branch-and-bound pruning. Compared
               with other rule-based optimization systems, it provides complete
               data model independence and more natural extensibility",
  pages     = "209--218",
  month     =  apr,
  year      =  1993,
  keywords  = "data structures;database management systems;dynamic
               programming;natural sciences computing;object-oriented
               databases;query processing;search problems;sorting;EXODUS
               optimizer generator;Volcano optimizer generator;branch-and-bound
               pruning;cost models;data model independence;dynamic
               programming;efficient search;extensibility;goal-directed
               searching;logical algebra;object-oriented DBMS;optimization
               rules;optimizer source code;physical algebra;query
               processing;request processing;rule-based optimization
               systems;scientific database systems;search engine;sorting
               order;Algebra;Costs;Data models;Database systems;Object oriented
               modeling;Power generation;Power system
               modeling;Prototypes;Search engines;Volcanoes"
}

@INPROCEEDINGS{Raman2008-gi,
  title     = "{Constant-Time} Query Processing",
  booktitle = "Data Engineering, 2008. {ICDE} 2008. {IEEE} 24th International
               Conference on",
  author    = "Raman, V and Swart, G and Qiao, Lin and Reiss, F and Dialani, V
               and Kossmann, D and Narang, I and Sidle, R",
  abstract  = "Query performance in current systems depends significantly on
               tuning: how well the query matches the available indexes,
               materialized views etc. Even in a well tuned system, there are
               always some queries that take much longer than others. This
               frustrates users who increasingly want consistent response times
               to ad hoc queries. We argue that query processors should instead
               aim for constant response times for all queries, with no
               assumption about tuning. We present Blink, our first attempt at
               this goal, that runs every query as a table scan over a fully
               denormalized database, with hash group-by done along the way. To
               make this scan efficient, Blink uses a novel compression scheme
               that horizontally partitions tuples by frequency, thereby
               compressing skewed data almost down to entropy, even while
               producing long runs of fixed-length, easily-parseable values. We
               also present a scheme for evaluating a conjunction of range and
               equality predicates in SIMD fashion over compressed tuples, and
               different schemes for efficient hash-based aggregation within
               the L2 cache. A experimental study with a suite of arbitrary
               single block SQL queries over a TPCH-like schema suggests that
               constant-time queries can be efficient.",
  pages     = "60--69",
  month     =  apr,
  year      =  2008,
  keywords  = "query processing;Blink;compression scheme;constant-time query
               processing;hash-based aggregation;Costs;Data
               structures;Databases;Delay;Entropy;Frequency;Hardware;Humans;Query
               processing;Tuning"
}

@ARTICLE{Neumann2011-uq,
  title     = "Efficiently compiling efficient query plans for modern hardware",
  author    = "Neumann, Thomas",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  4,
  number    =  9,
  pages     = "539--550",
  month     =  "1~" # jun,
  year      =  2011
}

@INPROCEEDINGS{Zukowski2006-oz,
  title     = "{Super-Scalar} {RAM-CPU} Cache Compression",
  booktitle = "Data Engineering, 2006. {ICDE} '06. Proceedings of the 22nd
               International Conference on",
  author    = "Zukowski, M and Heman, S and Nes, N and Boncz, P",
  abstract  = "High-performance data-intensive query processing tasks like
               OLAP, data mining or scientific data analysis can be severely
               I/O bound, even when high-end RAID storage systems are used.
               Compression can alleviate this bottleneck only if encoding and
               decoding speeds significantly exceed RAID I/O bandwidth. For
               this purpose, we propose three new versatile compression schemes
               (PDICT, PFOR, and PFOR-DELTA) that are specifically designed to
               extract maximum IPC from modern CPUs. We compare these
               algorithms with compression techniques used in (commercial)
               database and information retrieval systems. Our experiments on
               the MonetDB/X100 database system, using both DSM and PAX disk
               storage, show that these techniques strongly accelerate TPC-H
               performance to the point that the I/O bottleneck is eliminated.",
  pages     = "59--59",
  month     =  apr,
  year      =  2006,
  keywords  = "Bandwidth;Costs;Data analysis;Data
               mining;Databases;Encoding;Hardware;Information retrieval;Query
               processing;Throughput"
}

@ARTICLE{Johnson2008-cp,
  title     = "Row-wise Parallel Predicate Evaluation",
  author    = "Johnson, Ryan and Raman, Vijayshankar and Sidle, Richard and
               Swart, Garret",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  1,
  number    =  1,
  pages     = "622--634",
  month     =  aug,
  year      =  2008
}

@ARTICLE{Moerkotte1998-pd,
  title    = "Small materialized aggregates : a light weight index structure
              for data warehousing",
  author   = "Moerkotte, Guido",
  abstract = "Small Materialized Aggregates (SMAs for short) are considered a
              highly flexible and versatile alternative for materialized data
              cubes. The basic idea is to compute many aggregate values for
              small to medium-sized buckets of tuples. These aggregates are
              then used to speed up query processing. We present the general
              idea and present an application of SMAs to the TPC-D benchmark.
              We show that application of SMAs to TPC-D Query 1 results in a
              speed up of two orders of magnitude. Then, we elaborate on the
              problem of query processing in the presence of SMAs. Last, we
              briefly discuss some further tuning possibilities for SMAs.",
  journal  = "Proceedings VLDB Endowment",
  month    =  "21~" # jan,
  year     =  1998
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Barber2012-xt,
  title    = "Business Analytics in (a) Blink",
  author   = "Barber, Ronald and Bendel, Peter and Czech, Marco and Draese,
              Oliver and Ho, Frederick",
  abstract = "The Blink project’s ambitious goal is to answer all Business
              Intelligence (BI) queries in mere seconds, regardless of the
              database size, with an extremely low total cost of ownership.
              Blink is a new DBMS aimed primarily at read-mostly BI query
              processing that exploits scale-out of commodity multi-core
              processors and cheap DRAM to retain a (copy of a) data mart
              completely in main memory. Additionally, it exploits proprietary
              compression technology and cache-conscious algorithms that reduce
              memory bandwidth consumption and allow most SQL query processing
              to be performed on the compressed data. Blink always scans
              (portions of) the data mart in parallel on all nodes, without
              using any indexes or materialized views, and without any query
              optimizer to choose among them. The Blink technology has thus far
              been incorporated into two IBM accelerator products generally
              available since March 2011. We are now working on the next
              generation of Blink, which will significantly expand the ``sweet
              spot'' of the Blink technology to much larger, disk-based
              warehouses and allow Blink to ``own'' the data, rather than
              copies of it.",
  journal  = "IEEE",
  year     =  2012
}

@ARTICLE{Primsch2011-ij,
  title   = "{SAP} {HANA} Database - Data Management for Modern Business
             Applications",
  author  = "Primsch, J{\"{u}}rgen",
  journal = "SIGMOD Record",
  month   =  dec,
  year    =  2011
}

@INPROCEEDINGS{Boncz2005-wj,
  title     = "{MonetDB/X100}: {Hyper-Pipelining} Query Execution",
  booktitle = "{CIDR}",
  author    = "Boncz, Peter A and Zukowski, Marcin and Nes, Niels",
  volume    =  5,
  pages     = "225--237",
  year      =  2005
}

@ARTICLE{Willhalm2013-ri,
  title   = "Vectorizing Database Column Scans with Complex Predicates",
  author  = "Willhalm, Thomas and Oukidyz, Ismail and Mullery, Ingo and
             Faerber, Franz",
  journal = "ADMS",
  year    =  2013
}

@ARTICLE{Willhalm2009-hu,
  title     = "{SIMD-scan}: ultra fast in-memory table scan using on-chip
               vector processing units",
  author    = "Willhalm, Thomas and Popovici, Nicolae and Boshmaf, Yazan and
               Plattner, Hasso and Zeier, Alexander and Schaffner, Jan",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  2,
  number    =  1,
  pages     = "385--394",
  month     =  "1~" # aug,
  year      =  2009
}

@ARTICLE{Raman2013-em,
  title     = "{DB2} with {BLU} Acceleration: So Much More Than Just a Column
               Store",
  author    = "Raman, Vijayshankar and Attaluri, Gopi and Barber, Ronald and
               Chainani, Naresh and Kalmuk, David and KulandaiSamy, Vincent and
               Leenstra, Jens and Lightstone, Sam and Liu, Shaorong and Lohman,
               Guy M and Malkemus, Tim and Mueller, Rene and Pandis, Ippokratis
               and Schiefer, Berni and Sharpe, David and Sidle, Richard and
               Storm, Adam and Zhang, Liping",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  6,
  number    =  11,
  pages     = "1080--1091",
  month     =  aug,
  year      =  2013
}

@INCOLLECTION{Lemke2010-is,
  title     = "Speeding Up Queries in Column Stores",
  booktitle = "Data Warehousing and Knowledge Discovery",
  author    = "Lemke, Christian and Sattler, Kai-Uwe and Faerber, Franz and
               Zeier, Alexander",
  publisher = "Springer Berlin Heidelberg",
  pages     = "117--129",
  series    = "Lecture Notes in Computer Science",
  month     =  "30~" # aug,
  year      =  2010
}

@ARTICLE{Farber2012-vh,
  title   = "The {SAP} {HANA} Database -- An Architecture Overview",
  author  = "Farber, Franz and May, Norman and Lehner, Wolfgang and Gro\ss{}e,
             Philipp and Rauhe, Ingo Muller Hannes and Ag, Jonathan Dees Sap",
  journal = "IEEE",
  year    =  2012
}

@INPROCEEDINGS{Larson2013-mc,
  title     = "Enhancements to {SQL} Server Column Stores",
  booktitle = "Proceedings of the 2013 {ACM} {SIGMOD} International Conference
               on Management of Data",
  author    = "Larson, Per-Ake and Clinciu, Cipri and Fraser, Campbell and
               Hanson, Eric N and Mokhtar, Mostafa and Nowakiewicz, Michal and
               Papadimos, Vassilis and Price, Susan L and Rangarajan, Srikumar
               and Rusanu, Remus and Saubhasik, Mayukh",
  publisher = "ACM",
  pages     = "1159--1168",
  series    = "SIGMOD '13",
  year      =  2013,
  address   = "New York, NY, USA",
  keywords  = "column store, columnar storage, data warehousing, index, olap"
}

@ARTICLE{Psaroudakis2013-fn,
  title   = "Task Scheduling for Highly Concurrent Analytical and Transactional
             {Main-Memory} Workloads",
  author  = "Psaroudakis, Iraklis and Scheuer, Tobias",
  journal = "EPFL Infoscience",
  month   =  "26~" # aug,
  year    =  2013
}

@INCOLLECTION{Psaroudakis2014-ma,
  title     = "Scaling Up Mixed Workloads: A Battle of Data Freshness,
               Flexibility, and Scheduling",
  booktitle = "Performance Characterization and Benchmarking. Traditional to
               Big Data",
  author    = "Psaroudakis, Iraklis and Wolf, Florian and May, Norman and
               Neumann, Thomas and B{\"{o}}hm, Alexander and Ailamaki,
               Anastasia and Sattler, Kai-Uwe",
  publisher = "Springer International Publishing",
  pages     = "97--112",
  series    = "Lecture Notes in Computer Science",
  month     =  "1~" # sep,
  year      =  2014
}

@INPROCEEDINGS{Ratanaworabhan2006-jb,
  title     = "Fast lossless compression of scientific floating-point data",
  booktitle = "Data Compression Conference, 2006. {DCC} 2006. Proceedings",
  author    = "Ratanaworabhan, P and Ke, J and Burtscher, M",
  abstract  = "In scientific computing environments, large amounts of
               floating-point data often need to be transferred between
               computers as well as to and from storage devices. Compression
               can reduce the number of bits that need to be transferred and
               stored. However, the run-time overhead due to compression may be
               undesirable in high-performance settings where short
               communication latencies and high bandwidths are essential. This
               paper describes and evaluates a new compression algorithm that
               is tailored to such environments. It typically compresses
               numeric floating-point values better and faster than other
               algorithms do. On our data sets, it achieves compression ratios
               between 1.2 and 4.2 as well as compression and decompression
               throughputs between 2.8 and 5.9 million 64-bit double-precision
               numbers per second on a 3 GHz Pentium 4 machine.",
  pages     = "133--142",
  month     =  mar,
  year      =  2006,
  keywords  = "data compression;Pentium 4 machine;lossless compression;run-time
               overhead;scientific floating-point data;short communication
               latencies;storage devices;Analytical
               models;Bandwidth;Compression algorithms;Computational
               modeling;Computer simulation;Concurrent computing;Data
               engineering;Delay;Libraries;Registers"
}

@ARTICLE{Lamb2012-kg,
  title         = "The Vertica Analytic Database: {C-Store} 7 Years Later",
  author        = "Lamb, Andrew and Fuller, Matt and Varadarajan, Ramakrishna
                   and Tran, Nga and Vandier, Ben and Doshi, Lyric and Bear,
                   Chuck",
  abstract      = "This paper describes the system architecture of the Vertica
                   Analytic Database (Vertica), a commercialization of the
                   design of the C-Store research prototype. Vertica
                   demonstrates a modern commercial RDBMS system that presents
                   a classical relational interface while at the same time
                   achieving the high performance expected from modern ``web
                   scale'' analytic systems by making appropriate architectural
                   choices. Vertica is also an instructive lesson in how
                   academic systems research can be directly commercialized
                   into a successful product.",
  month         =  "21~" # aug,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DB",
  eprint        = "1208.4173"
}

@ARTICLE{Pelkonen2015-ko,
  title     = "Gorilla: A Fast, Scalable, In-memory Time Series Database",
  author    = "Pelkonen, Tuomas and Franklin, Scott and Teller, Justin and
               Cavallaro, Paul and Huang, Qi and Meza, Justin and
               Veeraraghavan, Kaushik",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  8,
  number    =  12,
  pages     = "1816--1827",
  month     =  aug,
  year      =  2015
}

@ARTICLE{Psaroudakis2015-lc,
  title     = "Scaling Up Concurrent Main-memory Column-store Scans: Towards
               Adaptive {NUMA-aware} Data and Task Placement",
  author    = "Psaroudakis, Iraklis and Scheuer, Tobias and May, Norman and
               Sellami, Abdelkader and Ailamaki, Anastasia",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  8,
  number    =  12,
  pages     = "1442--1453",
  month     =  aug,
  year      =  2015
}


@ARTICLE{Barber2014-ey,
  title     = "Memory-efficient Hash Joins",
  author    = "Barber, R and Lohman, G and Pandis, I and Raman, V and Sidle, R
               and Attaluri, G and Chainani, N and Lightstone, S and Sharpe, D",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  8,
  number    =  4,
  pages     = "353--364",
  month     =  dec,
  year      =  2014
}

@ARTICLE{Sidlauskas2014-ef,
  title     = "Spatial Joins in Main Memory: Implementation Matters!",
  author    = "\v{S}idlauskas, Darius and Jensen, Christian S",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  8,
  number    =  1,
  pages     = "97--100",
  month     =  sep,
  year      =  2014
}

@ARTICLE{Graefe2014-ds,
  title     = "In-memory Performance for Big Data",
  author    = "Graefe, Goetz and Volos, Haris and Kimura, Hideaki and Kuno,
               Harumi and Tucek, Joseph and Lillibridge, Mark and Veitch,
               Alistair",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  8,
  number    =  1,
  pages     = "37--48",
  month     =  sep,
  year      =  2014
}

@MISC{Russo_undated-ey,
  title        = "Optimizing High Cardinality Columns in {VertiPaq} - {SQLBI}",
  author       = "Russo, Marco",
  abstract     = "VertiPaq is the internal column-based database engine used by
                  PowerPivot and BISM Tabular models. High cardinality columns
                  might be the more expensive parts",
  howpublished = "\url{http://www.sqlbi.com/articles/optimizing-high-cardinality-columns-in-vertipaq/}",
  note         = "Accessed: 2015-10-7"
}

@UNPUBLISHED{Ferrari2012-hm,
  title  = "{VertiPaq} vs {ColumnStore} Comparison",
  author = "Ferrari, Alberto",
  month  =  aug,
  year   =  2012
}

@MISC{Jorgensen2013-of,
  title        = "Understanding the {SQL} Server Columnstore Index",
  booktitle    = "{SolarWinds} - {LogicalRead}",
  author       = "Jorgensen, Adam and Wort, Steven and LoForte, Ross and
                  Knight, Brian",
  abstract     = "Learn about SQL Server columnstore index, a column-based
                  non-clustered index to increase query performance for
                  workloads that involve large amounts of data.",
  month        =  "24~" # jun,
  year         =  2013,
  howpublished = "\url{http://logicalread.solarwinds.com/sql-server-columnstore-index-w02/}",
  note         = "Accessed: 2015-10-7"
}

@MISC{noauthor_undated-tc,
  title        = "Columnstore Indexes for Operational Analytics",
  howpublished = "\url{https://msdn.microsoft.com/en-us/library/dn817827.aspx}",
  note         = "Accessed: 2015-10-7"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@TECHREPORT{Delaney2014-ip,
  title    = "{SQL} Server {In-Memory} {OLTP} Internals Overview",
  author   = "Delaney, Kalen",
  abstract = "In-Memory OLTP (project ``Hekaton'') is a new database engine
              component, fully integrated into SQL Server. It is optimized for
              OLTP workloads accessing memory resident data. In-Memory OLTP
              allows OLTP workloads to achieve significant improvements in
              performance, and reduction in processing time. Tables can be
              declared as ‘memory optimized’ to enable In-Memory OLTP’s
              capabilities. Memory-optimized tables are fully transactional and
              can be accessed using Transact-SQL. Transact-SQL stored
              procedures can be compiled to machine code for further
              performance improvements on memory-optimized tables. The engine
              is designed for high concurrency and blocking is minimal.",
  month    =  mar,
  year     =  2014
}

@ARTICLE{Ramamurthy2003-cb,
  title     = "A Case for Fractured Mirrors",
  author    = "Ramamurthy, Ravishankar and DeWitt, David J and Su, Qi",
  journal   = "VLDB J.",
  publisher = "Springer-Verlag New York, Inc.",
  volume    =  12,
  number    =  2,
  pages     = "89--101",
  month     =  aug,
  year      =  2003,
  address   = "Secaucus, NJ, USA",
  keywords  = "Data placement, Disk mirroring, Vertical partitioning"
}

@ARTICLE{DeWitt1992-ki,
  title     = "Parallel database systems: the future of high performance
               database systems",
  author    = "DeWitt, David and Gray, Jim",
  journal   = "Commun. ACM",
  publisher = "ACM",
  volume    =  35,
  number    =  6,
  pages     = "85--98",
  month     =  "1~" # jun,
  year      =  1992,
  keywords  = "parallel database systems; parallel processing systems;
               parallelism"
}

@ARTICLE{Dean2004-cl,
  title   = "{MapReduce}: Simplied Data Processing on Large Clusters",
  author  = "Dean, Jeffrey and Ghemawat, Sanjay",
  journal = "OSDI",
  year    =  2004
}

@INPROCEEDINGS{Antoshenkov1996-jb,
  title     = "Order preserving string compression",
  booktitle = "Data Engineering, 1996. Proceedings of the Twelfth International
               Conference on",
  author    = "Antoshenkov, G and Lomet, D and Murray, J",
  abstract  = "Order-preserving compression can improve sorting and searching
               performance, and hence the performance of database systems. We
               describe a new parsing (tokenization) technique that can be
               applied to variable-length ``keys'', producing substantial
               compression. It can both compress and decompress data,
               permitting variable lengths for dictionary entries and
               compressed forms. The key notion is to partition the space of
               strings into ranges, encoding the common prefix of each range.
               We illustrate our method with padding character compression for
               multi-field keys, demonstrating the dramatic gains possible. A
               specific version of the method has been implemented in Digital's
               Rdb relational database system to enable effective multi-field
               compression",
  pages     = "655--663",
  month     =  feb,
  year      =  1996,
  keywords  = "data compression;encoding;relational databases;sorting;Digital
               Rdb relational database system;compressed forms;data
               decompression;database systems performance;multi-field
               compression;multi-field keys;order-preserving string
               compression;padding character compression;parsing
               technique;range common prefix encoding;searching
               performance;sorting performance;string-space
               partitioning;tokenization technique;variable-length dictionary
               entries;variable-length keys;Arithmetic;Binary trees;Data
               compression;Database
               systems;Dictionaries;Encoding;Frequency;Probability;Relational
               databases"
}

@INPROCEEDINGS{Piros2012-ok,
  title     = "Compression method for binary tree like bitmaps",
  booktitle = "2012 8th International Symposium on Mechatronics and its
               Applications",
  author    = "Piros, Sandor J and Peter, Korondi",
  year      =  2012
}

@INPROCEEDINGS{Moffat1992-tz,
  title     = "Parameterised compression for sparse bitmaps",
  booktitle = "Proceedings of the 15th annual international {ACM} {SIGIR}
               conference on Research and development in information retrieval",
  author    = "Moffat, Alistair and Zobel, Justin",
  publisher = "ACM",
  pages     = "274--285",
  month     =  "1~" # jun,
  year      =  1992
}

@ARTICLE{Abadi2006-bf,
  title   = "Integrating Compression and Execution in {Column-Oriented}
             Database Systems",
  author  = "Abadi, Daniel J and Madden, Samuel R and Ferreira, Miguel C",
  journal = "ACM SIGMOD Record",
  month   =  jun,
  year    =  2006
}

@ARTICLE{Westmann2000-mz,
  title     = "The implementation and performance of compressed databases",
  author    = "Westmann, Till and Kossmann, Donald and Helmer, Sven and
               Moerkotte, Guido",
  journal   = "ACM SIGMOD Record",
  publisher = "ACM",
  volume    =  29,
  number    =  3,
  pages     = "55--67",
  month     =  "1~" # sep,
  year      =  2000
}

@INPROCEEDINGS{Graefe_undated-uo,
  title      = "Data compression and database performance",
  booktitle  = "[Proceedings] 1991 Symposium on Applied Computing",
  author     = "Graefe, G and Shapiro, L D",
  publisher  = "IEEE Comput. Soc. Press",
  pages      = "22--27",
  conference = "1991 Symposium on Applied Computing"
}

@ARTICLE{Bloom1970-nr,
  title     = "{Space/Time} Trade-offs in Hash Coding with Allowable Errors",
  author    = "Bloom, Burton H",
  journal   = "Commun. ACM",
  publisher = "ACM",
  volume    =  13,
  number    =  7,
  pages     = "422--426",
  month     =  jul,
  year      =  1970,
  address   = "New York, NY, USA",
  keywords  = "hash addressing, hash coding, retrieval efficiency, retrieval
               trade-offs, scatter storage, searching, storage efficiency,
               storage layout"
}

@INPROCEEDINGS{Blanas2010-iz,
  title     = "A Comparison of Join Algorithms for Log Processing in
               {MaPreduce}",
  booktitle = "Proceedings of the 2010 {ACM} {SIGMOD} International Conference
               on Management of Data",
  author    = "Blanas, Spyros and Patel, Jignesh M and Ercegovac, Vuk and Rao,
               Jun and Shekita, Eugene J and Tian, Yuanyuan",
  publisher = "ACM",
  pages     = "975--986",
  series    = "SIGMOD '10",
  year      =  2010,
  address   = "New York, NY, USA",
  keywords  = "analytics, hadoop, join processing, mapreduce"
}

@ARTICLE{Oracle2015-fs,
  title   = "Oracle Database {In-Memory}",
  author  = "{Oracle}",
  journal = "Oracle White Paper",
  month   =  07,
  year    =  2015
}

@ARTICLE{Michael2007-if,
  title   = "Improving distributed join efficiency with extended bloom filter
             operations",
  author  = "Michael, Loizos and Nejdl, Wolfgang and Papapetrou, Odysseas and
             Siberski, Wolf",
  journal = "IEEE",
  year    =  2007
}

@ARTICLE{Holloway2008-rr,
  title     = "Read-optimized databases, in depth",
  author    = "Holloway, Allison L and DeWitt, David J",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  1,
  number    =  1,
  pages     = "502--513",
  month     =  "1~" # aug,
  year      =  2008
}

@INPROCEEDINGS{Abadi2008-dd,
  title     = "Column-stores vs. row-stores: how different are they really?",
  booktitle = "Proceedings of the 2008 {ACM} {SIGMOD} international conference
               on Management of data",
  author    = "Abadi, Daniel J and Madden, Samuel R and Hachem, Nabil",
  publisher = "ACM",
  pages     = "967--980",
  month     =  "9~" # jun,
  year      =  2008,
  keywords  = "c-store; column-oriented dbms; column-store; compression;
               invisible join; tuple materialization; tuple reconstruction"
}

@MISC{Abadi_undated-xk,
  title        = "{DBMS} Musings: Distinguishing Two Major Types of
                  {Column-Stores}",
  author       = "Abadi, Daniel and Profile, View my Complete",
  howpublished = "\url{http://dbmsmusings.blogspot.no/2010/03/distinguishing-two-major-types-of_29.html}",
  note         = "Accessed: 2015-9-28"
}

@MISC{Wikipedia_contributors2015-ut,
  title        = "Join ({SQL})",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "A SQL join clause combines records from two or more tables in
                  a relational database. It creates a set that can be saved as
                  a table or used as it is. A JOIN is a means for combining
                  fields from two tables (or more) by using values common to
                  each. ANSI-standard SQL specifies five types of JOIN: INNER,
                  LEFT OUTER, RIGHT OUTER, FULL OUTER and CROSS. As a special
                  case, a table (base table, view, or joined table) can JOIN to
                  itself in a self-join.",
  month        =  "20~" # sep,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Join_(SQL)&oldid=681927014}",
  note         = "Accessed: 2015-9-25"
}

@BOOK{Boncz2002-yj,
  title  = "Monet; a next-Generation {DBMS} Kernel For {Query-Intensive}
            Applications",
  author = "Boncz, Peter Alexander",
  year   =  2002
}

@MISC{Wikipedia_contributors2015-kh,
  title        = "{XQuery}",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "XQuery is a query and functional programming language that
                  queries and transforms collections of structured and
                  unstructured data, usually in the form of XML, text and with
                  vendor-specific extensions for other data formats (JSON,
                  binary, etc.). The language is developed by the XML Query
                  working group of the W3C. The work is closely coordinated
                  with the development of XSLT by the XSL Working Group; the
                  two groups share responsibility for XPath, which is a subset
                  of XQuery.",
  month        =  "29~" # aug,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=XQuery&oldid=678437271}",
  note         = "Accessed: 2015-9-25"
}

@ARTICLE{Manegold2000-st,
  title     = "Optimizing Database Architecture for the New Bottleneck: Memory
               Access",
  author    = "Manegold, Stefan and Boncz, Peter A and Kersten, Martin L",
  journal   = "VLDB J.",
  publisher = "Springer-Verlag New York, Inc.",
  volume    =  9,
  number    =  3,
  pages     = "231--246",
  month     =  dec,
  year      =  2000,
  address   = "Secaucus, NJ, USA",
  keywords  = "Decomposed storage model, Implementation techniques, Join
               algorithms, Main-memory databases, Memory access optimization,
               Query processing"
}

@INPROCEEDINGS{Lahiri2015-mz,
  title     = "Oracle Database {In-Memory}: A dual format in-memory database",
  booktitle = "Data Engineering ({ICDE)}, 2015 {IEEE} 31st International
               Conference on",
  author    = "Lahiri, T and Chavan, S and Colgan, M and Das, D and Ganesh, A
               and Gleeson, M and Hase, S and Holloway, A and Kamp, J and Lee,
               Teck-Hua and Loaiza, J and Macnaughton, N and Marwah, V and
               Mukherjee, N and Mullick, A and Muthulingam, S and Raja, V and
               Roth, M and Soylemez, E and Zait, M",
  abstract  = "The Oracle Database In-Memory Option allows Oracle to function
               as the industry-first dual-format in-memory database. Row
               formats are ideal for OLTP workloads which typically use indexes
               to limit their data access to a small set of rows, while column
               formats are better suited for Analytic operations which
               typically examine a small number of columns from a large number
               of rows. Since no single data format is ideal for all types of
               workloads, our approach was to allow data to be simultaneously
               maintained in both formats with strict transactional consistency
               between them.",
  pages     = "1253--1258",
  month     =  apr,
  year      =  2015,
  keywords  = "data mining;database indexing;information retrieval;storage
               management;transaction processing;OLTP workload;Oracle database
               in-memory;analytic operation;column format;data access;dual
               format in-memory database;index;row format;transactional
               consistency;Acceleration;Buffer storage;Indexes;Memory
               management;Optimization;Servers"
}

@ARTICLE{Elmqvist2013-cy,
  title   = "Ubiquitous Analytics: Interacting with Big Data Anywhere, Anytime",
  author  = "Elmqvist, Niklas and Irani, Pourang",
  journal = "Computer",
  month   =  apr,
  year    =  2013
}

@INPROCEEDINGS{Boncz2006-md,
  title     = "{MonetDB/XQuery}: A Fast {XQuery} Processor Powered by a
               Relational Engine",
  booktitle = "Proceedings of the 2006 {ACM} {SIGMOD} International Conference
               on Management of Data",
  author    = "Boncz, Peter and Grust, Torsten and van Keulen, Maurice and
               Manegold, Stefan and Rittinger, Jan and Teubner, Jens",
  publisher = "ACM",
  pages     = "479--490",
  series    = "SIGMOD '06",
  year      =  2006,
  address   = "New York, NY, USA",
  keywords  = "RDBMS, XML, XQuery, performance, relational XQuery systems,
               scalability"
}

@INPROCEEDINGS{Stonebraker2005-qz,
  title     = "C-store: A Column-oriented {DBMS}",
  booktitle = "Proceedings of the 31st International Conference on Very Large
               Data Bases",
  author    = "Stonebraker, Mike and Abadi, Daniel J and Batkin, Adam and Chen,
               Xuedong and Cherniack, Mitch and Ferreira, Miguel and Lau,
               Edmond and Lin, Amerson and Madden, Sam and O'Neil, Elizabeth
               and O'Neil, Pat and Rasin, Alex and Tran, Nga and Zdonik, Stan",
  publisher = "VLDB Endowment",
  pages     = "553--564",
  series    = "VLDB '05",
  year      =  2005,
  address   = "Trondheim, Norway"
}

@ARTICLE{Mukherjee2015-ul,
  title     = "Distributed Architecture of Oracle Database In-memory",
  author    = "Mukherjee, Niloy and Chavan, Shasank and Colgan, Maria and Das,
               Dinesh and Gleeson, Mike and Hase, Sanket and Holloway, Allison
               and Jin, Hui and Kamp, Jesse and Kulkarni, Kartik and Lahiri,
               Tirthankar and Loaiza, Juan and Macnaughton, Neil and Marwah,
               Vineet and Mullick, Atrayee and Witkowski, Andy and Yan, Jiaqi
               and Zait, Mohamed",
  journal   = "Proceedings VLDB Endowment",
  publisher = "VLDB Endowment",
  volume    =  8,
  number    =  12,
  pages     = "1630--1641",
  month     =  aug,
  year      =  2015
}

@MISC{Zicari2012-is,
  title        = "In-memory database systems. Interview with Steve Graves,
                  {McObject}",
  booktitle    = "{ODBMS} Industry Watch",
  author       = "Zicari, Roberto V",
  month        =  "16~" # mar,
  year         =  2012,
  howpublished = "\url{http://www.odbms.org/blog/2012/03/in-memory-database-systems-interview-with-steve-graves-mcobject/}",
  note         = "Accessed: 2015-9-21"
}

@MISC{noauthor_undated-vw,
  title        = "Microsoft {In-Memory} Move Challenges {SAP}, Oracle -
                  {InformationWeek}",
  booktitle    = "{InformationWeek}",
  abstract     = "Microsoft SQL Server Project Hekaton promises in-memory
                  transactional processing that will stack up against SAP Hana
                  and Oracle Exadata.",
  howpublished = "\url{http://www.informationweek.com/software/information-management/microsoft-in-memory-move-challenges-sap-oracle/d/d-id/1107262?}",
  note         = "Accessed: 2015-9-21"
}

@MISC{Bort2013-fb,
  title        = "{IBM} And {SAP} Say You Shouldn't Believe Larry Ellison's
                  Latest Claims About Oracle's New Products",
  booktitle    = "Business Insider",
  author       = "Bort, Julie",
  abstract     = "SAP and IBM had something to say about Oracle's new products
                  that compete with them.",
  month        =  "23~" # sep,
  year         =  2013,
  howpublished = "\url{http://www.businessinsider.com/sap-and-ibm-respond-to-oracles-claims-2013-9}",
  note         = "Accessed: 2015-9-21"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Intelligist2012-vu,
  title        = "{SAP} {HANA} versus Microsoft Business Intelligence and
                  xVelocity, take two…",
  booktitle    = "business intelligist",
  author       = "Intelligist, Business",
  abstract     = "Not surprisingly, my original post generated huge traffic to
                  my blog as well as a couple of good comments. I am sorry that
                  it took me a while to respond, but now that it is Saturday
                  and I am at Starbucks,I don't really have any excuse not to
                  respond anymore… Before I jump into a…",
  month        =  "25~" # aug,
  year         =  2012,
  howpublished = "\url{http://businessintelligist.com/2012/08/25/sap-hana-versus-microsoft-business-intelligence-and-xvelocity-take-two/}",
  note         = "Accessed: 2015-9-21"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Intelligist2012-gb,
  title        = "{SAP} {HANA} versus Microsoft Business Intelligence and
                  xVelocity",
  booktitle    = "business intelligist",
  author       = "Intelligist, Business",
  abstract     = "This will be in a long rant format as I am still trying to
                  organize my thoughts around this subject. Before I start
                  comparing and contrasting the two vendors, I just wanted to
                  make a quick statement to get something off my chest:
                  Generally speaking, HANA is a good business decision for SAP
                  as it…",
  month        =  "7~" # aug,
  year         =  2012,
  howpublished = "\url{http://businessintelligist.com/2012/08/06/sap-hana-versus-microsoft-business-intelligence-and-xvelocity/}",
  note         = "Accessed: 2015-9-21"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Intelligist2012-xz,
  title        = "{SAP} Hana vs Microsoft {SQL} Server, the war is on.",
  booktitle    = "business intelligist",
  author       = "Intelligist, Business",
  abstract     = "and the Germans are making the first move... I have written
                  two posts about Hana vs. xVelocity (Part I and Part II)
                  making the basic point that there is nothing truly visionary
                  or revolutionary in SAP Hana from the technology perspective
                  and that a customer can get all the benefits (or rather
                  alleged benefits as…",
  month        =  "22~" # nov,
  year         =  2012,
  howpublished = "\url{http://businessintelligist.com/2012/11/21/sap-hana-vs-microsoft-sql-server-the-war-is-on/}",
  note         = "Accessed: 2015-9-21"
}

@MISC{noauthor_2012-ur,
  title        = "Pondering the Microsoft {SQL} Server Announcements - {SAP}
                  {HANA}",
  booktitle    = "{SAP} {HANA}",
  abstract     = "Microsoft has announced a new in-memory OLTP product
                  code-named Hekaton that will be available in 2014-2015. In
                  addition they have introduced some new columnar capabilities
                  in SQL Server 2012. The...",
  month        =  "21~" # nov,
  year         =  2012,
  howpublished = "\url{https://blogs.saphana.com/2012/11/21/pondering-the-microsoft-sql-server-announcements/}",
  note         = "Accessed: 2015-9-21"
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Eaton_undated-oi,
  title        = "{IBM} {DB2} 10.5 with {BLU} Acceleration vs Microsoft {SQL}
                  Server Columnar Index",
  author       = "Eaton, Chris",
  abstract     = "I’ve done some blogging now about BLU Acceleration and I have
                  a plan for more details to come in future posts. But first I
                  wanted to turn my ...",
  howpublished = "\url{http://it.toolbox.com/blogs/db2luw/ibm-db2-105-with-blu-acceleration-vs-microsoft-sql-server-columnar-index-57303}",
  note         = "Accessed: 2015-9-21"
}

@ARTICLE{Lee2012-wc,
  title     = "Parallel Data Processing with {MapReduce}: A Survey",
  author    = "Lee, Kyong-Ha and Lee, Yoon-Joon and Choi, Hyunsik and Chung,
               Yon Dohn and Moon, Bongki",
  journal   = "SIGMOD Rec.",
  publisher = "ACM",
  volume    =  40,
  number    =  4,
  pages     = "11--20",
  month     =  jan,
  year      =  2012,
  address   = "New York, NY, USA"
}

@ARTICLE{Chen2012-ah,
  title     = "Business Intelligence and Analytics: From Big Data to Big Impact",
  author    = "Chen, Hsinchun and Chiang, Roger H L and Storey, Veda C",
  abstract  = "... Page 7. Chen et al./Introduction: Business Intelligence
               Research ... from health big data poses significant research and
               practical challenges, especially considering the HIPAA (Health
               Insurance Portability and Accountability Act) and IRB
               (Institutional Review Board) requirements ...",
  journal   = "Miss. Q.",
  publisher = "hmchen.shidler.hawaii.edu",
  volume    =  36,
  number    =  4,
  pages     = "1165--1188",
  year      =  2012
}

@INPROCEEDINGS{Fontoura2011-xo,
  title     = "Efficiently Encoding Term Co-occurrences in Inverted Indexes",
  booktitle = "Proceedings of the 20th {ACM} International Conference on
               Information and Knowledge Management",
  author    = "Fontoura, Marcus and Gurevich, Maxim and Josifovski, Vanja and
               Vassilvitskii, Sergei",
  publisher = "ACM",
  pages     = "307--316",
  series    = "CIKM '11",
  year      =  2011,
  address   = "New York, NY, USA",
  keywords  = "bitmaps, inverted index, precomputation, term co-occurrence"
}

@INCOLLECTION{Wu2004-mp,
  title     = "On the Performance of Bitmap Indices for High Cardinality
               Attributes",
  booktitle = "Proceedings 2004 {VLDB} Conference",
  author    = "Wu, Kesheng and Otoo, Ekow and Shoshani, Arie",
  publisher = "Elsevier",
  pages     = "24--35",
  year      =  2004
}

@PHDTHESIS{Bjorklund2011-wh,
  title  = "Column Stores versus Search Engines and Applications to Search in
            Social Networks",
  author = "Bj\o{}rklund, Truls A",
  month  =  jan,
  year   =  2011,
  school = "Norwegian University of Science and Technology"
}


@MISC{Wikipedia_contributors2015-gt,
  title        = "Inverted index",
  booktitle    = "Wikipedia, The Free Encyclopedia",
  author       = "{Wikipedia contributors}",
  abstract     = "In computer science, an inverted index (also referred to as
                  postings file or inverted file) is an index data structure
                  storing a mapping from content, such as words or numbers, to
                  its locations in a database file, or in a document or a set
                  of documents (named in contrast to a Forward Index, which
                  maps from documents to content). The purpose of an inverted
                  index is to allow fast full text searches, at a cost of
                  increased processing when a document is added to the
                  database. The inverted file may be the database file itself,
                  rather than its index. It is the most popular data structure
                  used in document retrieval systems,[1] used on a large scale
                  for example in search engines. Additionally, several
                  significant general-purpose mainframe-based database
                  management systems have used inverted list architectures,
                  including ADABAS, DATACOM/DB, and Model 204.",
  month        =  "8~" # dec,
  year         =  2015,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Inverted_index&oldid=694251046}",
  note         = "Accessed: 2015-12-14"
}


