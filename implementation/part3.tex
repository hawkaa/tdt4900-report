\chapter{Implementation Part III: Column Operations and Other Cool Stuff}
\label{chap:Implementation Part III: Column Operations and Other Cool Stuff}
So far, we have seen that our implementation has given drastical reduction in memory usage and load time. In general, reduced memory usage normally results in higher program performance. However, we have not yet utilized our new column structure.

\section{Pointer Exposure}
\label{sec:Pointer Exposure}
The \bd~functionality in \gap~works strictly on arrays of floating point values (\cn{TArray<double>}). This buffer is created with a \cn{SourceMeasureProvider} class which is linked to a data descriptor. For every composition object in the data source, it calls the \fn{GetValue} method, which returns a \cn{GValue}. The buffer is then filled by calling \fn{GetAsFloat} on all values. 

Our observation is simple: If the value buffer is stored in a native column structure, the buffers already exist and its pointer can be passed to the \cn{SourceMeasureProvider} directly. As seen in Table X, the time it takes to generate source measure lookup is drastically reduced, or in fact removed.

\subsection{Creating the Array Anyway}
\label{sub:Creating the Array Anyway}
As seen in Table X, there are drastic differences between native floating point columns and everything else. For instance, the quantity-column, which is an integer, it takes \~ 1500ms to create a source measure index. The same is the case for tax, which is floating point but dictionary encoded. We hypothetize that the reason for this is the large amount of memory allocations for \cn{GValue}s and access patterns, and that we can do better by operating on the whole column in a batch.
\begin{lstlisting}
function CNativeRealFieldValueCollection.GetDoubleArray
: TArray<double> ;
begin
  Result := m_arrValues;
end;
\end{lstlisting}
We proceed to implement \fn{GetDoubleArray} in integer and dictionary columns. This is seen in the code above. As we see in Table X, this gives magnificent results.

\section{Lookup Index Creation}
\label{sec:Lookup Index Creation}
A common operation in \bd~is to create a lookup index from one table to another, that maps data source indexes from one table to data source indexes in another table. This could be considered as a join operation.

For a database, a typical to such function input is two columns. As we have read in the background section, one such algorithm, the \textit{nested loop algorithm}, will first create a dictionary on the smallest column and then iterate the largest column and insert indexes. However, in \gap, such dictionary already exists as an \textit{identifier index}. This is used several places in the application, like blablabla \todo{Find out where this dictionary is used}. This dictionary either exists already, or is created by another mechanism in the application

We may, therefore, use this dictionary as an input to the column.

\section{Identifier Index}
\label{sec:Identifier Index}
In the previous section, we saw that an identifier index is used in a join algorithm. However, in certain circumstances, this index is not created, and takes a while to generate. We implement such function in our columns. The results are shown in Table X.

\section{Column Selection}
\label{sec:Column Selection}
Now, we have four different \cn{FieldValueCollection} to chose from. Until now, which columns have been used have been selected by specifying which storage format should be used in the metadat. However, this disregards an important aspect of \mdd: The users should not need to know anything about the implementation to make good programs. In other words, it is up to \gap~to decide which storage format is best suited for the different use cases.

First, we observe that our column store has no negative impact on the application. Hence, we may safely assume that all Composition Objects may store their data using our new colum Composition Value Collection. Second, we have not experienced any performance reduction by using native data types. We may, therefore, assume that we can use this implementation whenever we can. Third, there is no negative performance impact on the bitpacked dictionary versus the non-packed one.

However, we have seen a significant space saving by chosing dictionary columns on low and medium cardinality columns. However, for some columns with high cardinality, a dictionary column will not save space, since most values are stored in the dictionary anyway. In addition, the load time of the columns (due to the linear dictionary lookup) has an exponential growth on such columns.

\afigure{img/gap-statistics.png}{Using statistics to select the proper storage format. A statistics provider service will, for a given Data Descriptor, return relevant statiscis, preferrably number of rows and number of unique values. The service might cache, restructure and calculate some of of the statistics manually, as the different database providers have different interfaces.}{fig:gap-statistics}{0.8}.
To overcome the above challenge, we propose using database statistics to select the proper column. We have implemented a statistics provider that will, for a given data descriptor, return relevant statistics for a column, including number of values and number of unique values.

\subsection{Data Source Filters}
\label{sub:Data Source Filters}
One of the major limitations in our statistics provider, is that it considers the entire dataset. However, it is quite common to add a filter on the data source for tasks or analyses. Typically, one would be interested in looking at sales for a given date range. In other words, decisions made about the storage format for the entire dataset might not be optimal for filtered subsets of the data.

One solution to this problem, would be to increase the granularity of the statistics. Instead of mapping a providing data descriptor to a statistics object, the provider should also accept a data source as an input. This would definetely increase the precision of the column selection algorithm. However, this approach does not come without challenges. First, the database backends has no support for this kind of statistics. Hence, the statistics would need to be calculated on-demand, either with custom queries (\texttt{COUNT} and \texttt{COUNT DISTINCT}) or by the application itself. Doing this before every data source load is time-consuming and would increase load time. 

This problem can be overcome if \gap~gather data source statistics after the column has been loaded and store the result. On the first load, the original statistics will be used. After the data is loaded, a low-priority process is started to generate the statistics for a given data source. On consequtive loads, the statistics returned by the provider will reflect the previous load of the data source, and will in most occasions be more precise than considering the column as a whole.

Another, but less precise way, is to extrapolate the statistics for a given filter. For instance:
\begin{itemize}
    \item Low-cardinality columns tend to have the same amount of unique values no matter how many more values are added (typically gender, age etc).
    \item High-cardinality columns tend to have a linear relationship between the number of values and the number of unique values.
    \item Some column types are more predictable than others. For instance, the number of distinct values in a code domain remains fixed, and some data types are ordered, like timestamps. 
\end{itemize}
The results of this extrapolation could also be dependendent on what kind of filter that is applied: Random, max or min values, or most occurences of different values.

\subsection{Other Use-Cases for Statistics}
\label{sub:Other Use-Cases for Statistics}
For obvious reasons, we may assume that the statistics are mere estimates. However, if we knew that the statistics were not only statistics but an exact measure of the column distribution, more optimizations could be applied. First and foremost, the exact buffer sizes, including dictionaries, could be allocated in the beginning, and not the doubling strategy explained in Section \ref{sub:FieldValueCollection growth strategy}. On every reallocation, there's a potential copy operation that copies the entire value buffer from one place to the other. Exact buffer sizes would reduce the risk of this. This is even more important for bit packed columns; on every dictionary overflow, the entire value buffer is rebuilt. 

\subsection{Special Cases}
\label{sub:Special Cases}





