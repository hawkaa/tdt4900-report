\chapter{Part III: Compression}
\label{chap:Compression}
So far we have reduced the bytes per \texttt{LINEITEM} from 715 to 374 in the \textit{TPC-H Q1 Data Load Benchmark}. This corresponds to 48 \%. However, we have yet not explored any compression techniques. In this chapter, we investigate the effects of light-weight comrpessions dictionary encoding and bitpacking. In addition to this, we implement \texttt{null}-pointer compression and store data in a packed fashion.

\clearpage

\section{Dictionary Encoding}
\label{sec:Dictionary Encoding}
% Short introduction on DE
In Section \ref{sec:Dictionary Encoding}, we saw that dictionary encoding is commonly used to store data in a database, especially read-optimized ones. Such encoding has several benefits, with the main being compression. Some operations are also speeded up by this storage format, especially string predicates. To do a select operation on a dictionary encoded column, the dictionary needs only to be scanned once to find the keys corresponding to the query, then the integer array is scanned. This reduces expensive string operations into cheap integer comparisons.

In a dictionary encoded column, the dictionary can be structured either for read or write performance. For read operations, there is a lookup from a dictionary index and the actual value. For such operations, implementing the dictionary as a list would suffice, adding new keys to the end of the list as they appear. Hence, lookups happen in constant time. However, with this implementation, assuming that the dictionary is not sorted, a linear search through the dictionary is triggered every time there is a write operation to the column, because the correct dictionary key must be found. 

\afigure{img/dict-inverse.png}{Dictionary Encoding. There exists a constant time lookup mechanism not only from dictionary keys to values, but also a lookup mechanism from values to dictionary keys.}{fig:dict-inverse}{0.8}
Since data sources in \gap~can be used in many different places, some write-intense, others read-intense, we implement a dictionary that can do both read and write operations in constant time. This comes at the cost of increased memory; a mapping from values to keys must be maintained. This is depicted in Figure \ref{fig:dict-inverse}. For the value-to-key lookup, a dictionary is used, such that lookups can be performed in constant time. When \fn{Consolidate} is called, the inverse dictionary lookup is discarded to save memory. 

The integer array, or value buffer, uses the same growth strategy as previous implementations. Like \cn{FieldValueCollection} and \cn{PrimitiveFieldValueCollection}, buffers are doubled on index overflow. The \fn{Consolidate} resizes the key array to the correct size after the data source has been loaded.

\afigure{img/PrimitiveDictionaryFieldValueCollection.png}{\cn{PrimitiveDictionaryFieldValueCollection} class diagram. The class inherits from \cn{FieldValueCollectionBase}.}{fig:PrimitiveDictionaryFieldValueCollection}{1.0}
We chose to implement the dicitonary and inverse lookup using \cn{TArray} and \cn{TDictionary} respectively. We also chose to store values as their primitive implementation, like we investigated in Chapter \ref{chap:storage-format}. The class inherits from \cn{FieldValueCollectionBase}, and uses the same primitive value helper delegate class as \cn{PrimitiveDictionaryFieldValueCollection}.

\section{Bitpacking}
\label{sec:Bitpacking}
% Short introduction on bp
Bitpacking is a compression technique where values are stored with no more bits than needed. As we saw in Section \ref{sub:Dictionary Encoding and Bitpacking}, dictionary encoding and bitpacking goes hand in hand. We, therefore, implement bitpacking in our dictionary encoded columns to save memory.

% Main implementation
\afigure{img/bp-overflow.png}{Bitpacking overflow. First, the value buffer is rebuilt to accomodate a new key in the dictionary that caused an overflow. Here, the array contents are copied to another buffer with extra padding. Second, if needed, a new cell is added for the new value.}{fig:bp-overflow}{1.0}
The value buffer is implemented as an array of 64-bits values which each value is denoted as a \textit{cell}. Unlike previous column implementations, a cell is added one at a time, instead of the doubling strategy. This was done for simplicity reasons. Depending on the number of bits used per value, this means that more values can be added per reallocation. On dictionary overflow, the new number of bits per value is calculated, and the entire value buffer is rebuilt. Both buffer and dictionary overflow is depicted in Figure \ref{fig:bp-overflow}. Like the non-packed version, \cn{PrimitivePackedDictionaryFieldValueCollection} uses an inverse lookup which are discarded on \fn{Consolidate}.

% Dictionary overflow
One of the main challenges with bitpacking, is word alignment. If keys are allowed to cross word boundaries, not only performance might degrade, but it increase implementation and code complexity. Therefore, we take a simpler approach, where the number of bits used to store each value must be a power of 2. As seen in Figure \ref{fig:bp-overflow}, a dictionary overflow causes the bits per value to go from two to four to avoid values stored across word boundaries.

\afigure{img/PrimitivePackedDictionaryFieldValueCollection.png}{\cn{PrimitivePackedDictionaryFieldValueCollection} class diagram. The class is very similar to \cn{PrimitiveDictionaryFieldValueCollection}, but it has a different storage structure for its values.}{fig:PrimitivePackedDictionaryFieldValueCollection}{0.7}
The bitpacked column class was implemented using a \cn{TArray<UInt64>} for storing data. Like the non-packed version, a \cn{TArray} was used for the dictionary and \cn{TDictionary} for the inverse lookup. In addition to this, it holds some state regarding the bitpacking, and a private function \fn{IncreaseDictionaryCapacity} which rebuilds the value buffer on dictionary overflow. The class diagram is seen in Figure \ref{fig:PrimitivePackedDictionaryFieldValueCollection}.

\missingfigure{Figur om hvordan man henter ut verdier med masken?}

\section{Null-Pointer Compression}
\label{sec:Null-Pointer Compression}
\afigure{img/null-pointer-compression.png}{Null-pointer compression. Based on the observation that most \cn{CompositionObject} properties are normally \texttt{null}, data can be compressed, since columns are not created before the properties are set.}{fig:null-pointer-compression}{0.7}
So far, we have compressed data from the Information System and Information System Development Layers. However, there are several other structures used by \cn{CompositionObject}s which are used in the Method Engineering layer. These structures include lists of data validation and integrety errors, formatting rules, and more. These attributes are not fundamentally different than attributes from the other model-driven engineering layers, which means we are able to put these pointers into our column store. Although one might initially think that we have only moved the problem, and that the pointers now are stored in the column store instead of at object-level, the compression is based on the observation that these pointers are normally \texttt{null}. This means that these columns does not need to be allocated before a value is actually set. An illustration of this is seen in Figure \ref{fig:null-pointer-compression}.

To enable null pointer compression, we implement a new generic column structure named \cn{DefaultObjectList<TType>}. This is an array structure that can be instantiated with any class type, that will return \texttt{nil} if no values are set. If values are set, the array is allocated. The \cn{CompositionValueCollection} are extended with such attributes for all properties needed in the \cn{CompositionObject}s with corresponding get and set methods that requires datasource index as input.

\section{Packing Data}
\label{sec:Packing Data}
\begin{delphicode}{Structure with packed data.}{lst:packed-struct}
PackedComObjData = packed record
    modifyCount,
    datasourceIndex    : integer; 
    state              : EObjectState;
    filterStatus       : EFilterStatus;
end;
\end{delphicode}
Some properties on a \cn{CompositionObject} are always set. This include a modified counter, some state variables, and of course the data source index. To reduce the memory footprint used by these attributes, they are packed in a struct using \delphi's \fn{packed} keyword. See Listing \ref{lst:packed-struct}. This packs the data and tells the compiler to disregard word boundaries \cite{noauthor_undated-vu}.

For this to work, the attributes is removed from the \cn{CompositionObject} class and moved to a \cn{PackedComObjData} record type. All read and write properties in \cn{CompositionObject} is rewritten to access this new structure.

After applying both null-pointer compression from Section \ref{sec:Null-Pointer Compression}, a \cn{CompositionObject} has no attributes except for a pointer to the column storage and a packed struct with essential state data. \todo{Vet dette ikke er helt sant, men kan jeg skrive det?}

\section{Test Results}
\label{sec:Test Results}
We test our implementation using Benchmark \ref{bm:q1} and Benchmark \ref{bm:write}. Like the previous two chapters, we are curious to see how compression techniques affect memory consumption, load time, read operations, and write operations. We test three implementations: Dictionary encoding, dictionary encoding with bitpacking, and dictionary encoding with bitpacking, null pointer compression, and packed object data.

\begin{table}
    \begin{tabularx}{\textwidth}{X | X X}
        & Distinct Values & Dictionary Encoding? \\ 
        \hline
        \hline
        LINEITEMKEY & 600,000 & No \\
        LINESTATUS & 3 & Yes \\
        RETURNFLAG & 3 & Yes \\
        SHIPDATE & 2526 & Yes \\
        QUANTITY & 50 & Yes \\
        EXTENDEDPRICE & 598,966 & No \\
        DISCOUNT & 11 & Yes \\
        TAX & 9 & Yes
    \end{tabularx}
    \caption{Column selection for Benchmark \ref{bm:q1}. Low-cardinality columns are dictionary encoded, while the others are not. The numbers are based on Scaling Factor 0.1.}
    \label{tab:column-selection}
\end{table}
Like we read in Section \ref{sec:Dictionary Encoding}, dictionary encoding is effective when there is only a small number of distinct values compared to the number of total values. Hence, not all columns should be encoded with dictionary encoding. Therefore, certain properties for \texttt{LINEITEM} were picked by-hand to be dictionary encoded for this test, based on the column cardinality. The dictionary encoded columns with cardinalities are shown in Table \ref{tab:column-selection}. The table shows that most columns are low-cardinality, and will benefit from dictionary encoding. The numbers are based on the \texttt{LINEITEM} table in the \tpch, and the configuration is used in all benchmarks.

\subsection{TPC-H Result}
\label{sub:TPC-H Result}
Benchmark \ref{bm:q1}, the \textit{TPC-h Q1 Data Load Benchmark}, was run with all three compression configurations. In addition to this, dictionary encoding was run with raw value load, as explained in Section \ref{sec:Avoiding GValues at Load Time}. Both scaling factors were used for the tests. Like the previous chapters, all tests were run three times per configuration. There was low variance in the results, and no single measurement was more than 15 \% different from the average value.
\begin{table}
    \centering
    \begin{tabularx}{\textwidth}{X | X X}
        & SF0.01 & SF0.1 \\ 
        \hline
        \hline
        Primitive Column Store & 333 bytes & 374 bytes \\
        Dictionary Encoding & 298 bytes & 318 bytes \\
        Dictionary Encoding w/ raw value load & 298 bytes & 318 bytes \\
        Dictionary Encoding w/ bitpacking & 290 bytes & 301 bytes \\
        Dictionary Encoding w/ bitpacking, null-pointer compression, and attribute packing & 226 bytes & 237 bytes \\
    \end{tabularx}
    \caption{Bytes per \texttt{LINEITEM} for various compression implementations.} 
    \label{tab:dict-bpl}
\end{table}

We see in Table \ref{tab:dict-bpl} that memory used per lineitem is reduced by 11 \% and 15 \% for scaling factors 0.01 and 0.1 respectively. Also, there is no longer a difference between memory footprint for original and new raw value loading mechanism, something we observed in Section \ref{sub:storage-format-tpch-results}. There is a slight memory reduction by applying bit-packing, but null-pointer compression and attribute packing has reduced memory storage from 374 to 237 bytes for scaling factor 0.1, which means our compression has reduced memory usage by 37 \%. 
\begin{table}
    \centering
    \begin{tabularx}{\textwidth}{X | X X}
        & SF0.01 & SF0.1 \\ 
        \hline
        \hline
        Primitive Column Store & 1210 ms & 13585 ms \\
        Dictionary Encoding & 1332 ms & 14826 ms \\
        Dictionary Encoding w/ raw value load & 2215 ms & 21760 ms \\
        Dictionary Encoding w/ bitpacking & 1421 ms &  15366 ms \\
        Dictionary Encoding w/ bitpacking, null-pointer compression, and attribute packing & 1389 ms & 15584 ms \\
    \end{tabularx}
    \caption{Load times for Benchmark \ref{bm:q1} for different column compression implementations for scaling factors 0.01 and 0.1.} 
    \label{tab:dict-load}
\end{table}

As we see in Table \ref{tab:dict-load}, load times have increased as a result of the compression. For scaling factor 0.1, dictionary encoding adds 1.2 seconds load time, bitpacking adds an additional 0.5 seconds, and null-pointer compression and attribute packing adds 0.2 seconds more. Even though load time has been increased, it is still less than the original implementation. Loading raw values into the columns increases load time to 21.8 seconds for SF 0.1. We discuss this observation in Section \ref{sec:compression-discussion}.

We refrain from presenting the timing results for lookup index generation and source measure lookup from Benchmark \ref{bm:q1}. These does not vary significantly from the results presented in Section \ref{sec:storage-format-test-results}.

\subsection{Write Benchmark}
\label{sub:compression-write-benchmark}
\begin{table}
    \begin{tabularx}{\textwidth}{X | X X X X}
         & \texttt{QUANTITY} & \texttt{EXTENDEDPRICE} & \texttt{COMMENT} & \texttt{SHIPDATE}\\ 
        \hline
        \hline
        Primitive Column Store & 1807 ms & 1779 ms & 1820 ms & 2352 ms \\
        Dictionary Encoding & 1855 ms & 1774 ms & 1753 ms & 2183 ms \\
        Dictionary Encoding /w Bitpacking & 1757 ms & 1638 ms & 1657 ms & 2198 ms \\
        Dictionary Encoding /w Bitpacking, null-pointer compression and attribute packing & 1939 ms & 1813 ms & 1777 ms & 227 ms \\
    \end{tabularx}
    \caption{Test results for Benchmark \ref{bm:write}.}
    \label{tab:compression-write}
\end{table}
Write performance has not been significantly changed compared to primitive column storage, as seen it Table \ref{tab:compression-write}. Bitpacking is the fastest of all implementations, but only by very little.

\section{Discussion}
\label{sec:compression-discussion}
We are excited to see the effects data compression. For scaling factor 0.1, the memory used to store each \texttt{LINEITEM} is reduced by 37 \%. In this formula, dictionary encoding, null-pointer compression and attribute packing contribute the most, while bitpacking yields no more than 5 \% total memory savings. 

We are especially excited to see that compression in the Method Engineering layer, namely null-pointer compression and attribute packing, contribute to large memory savings. This has implications for all system development, not only in the realm of model-driven engineering. We elaborate on these implications further in Section \ref{X}.

By using dictionary encoding, we have been able to save the same amount of memory as the built-in caching mechanism for data source load. However, data load times are significantly higher by using our new raw data load mechanism. We believe this is because the caching mechanism not only reuse data, but also saves the job of converting XML string values to the primitive data type, a job that can be cumbersome for certain data types, like dates.

Write results are not significantly affected by compression, which might indicated that there is much more overhead associated with such operations than only data write and read. Although the results of Benchmark \ref{bm:write} show that write performance is reduced compared to the original implementation, this is no more than 10 \%.

\section{Chapter Conclusion}
\label{sec:Chapter Conclusion}
We have reduced memory footprint by 67 \% compared to the original implementation by applying compression techniques known by the database technology. Despite of the fact that load time is increased by compression, it is still lower than the original implementation by 33 \%.

After this chapter, we are left with two major challenges. First, currently, the correct column structure is picked by hand. However, this is not feasible in the long-run. Although such information could be added to the application model, we should investigate whether \gap~can pick the correct storage format without modeller interaction. We do so in Chapter X.

Second, and more important, is that we have applied techniques used in read-optimized databases, but so far only been hurt by our new implementation. For instance, the source measure lookup operation now takes 6.9 seconds, which originally only took 0.3 seconds. We claim that these read-intensive operations can be rewritten to utilize our new storage formats. We look into this in Chapter X.

\subsection{Future Work}
\label{sub:Future Work}
Like last chapter, we encourage studying the built-in caching mechanism on data source load. We have proved that dictionary encoded columns reuse values as efficiently as the already existing mechanism, since, as opposed to for pure primitive value columns, dictionary encoding use the same number of bytes per lineitem regardless of loading mechanisms. However, the time it takes to load the \tpch~Q1 data mart is still more than the original. We, therefore, encourage further scrutinization of the built-in caching mechanism, for instance by caching dictionary keys for raw string values instead of the column primitive type. This way, data conversion results can also be reused.

In the bitpacked column, cells are added one at a time. This was done for simplicity reasons, and the fact that adding a cell adds more than one value, reducing the number of memory allocations. However, the effects of doubling the array instead of adding one cell at a time should be investigated.


