\chapter{Part III: Compression}
\label{chap:compression}
So far we have reduced the number of bytes per \lineitem~from 715 bytes to 374 bytes in the \textit{TPC-H Q1 Data Load Bencmark}. This corresponds to a reduction of 48 \%. However, we have not yet explored any compression techniques which we studied in Chapter \ref{chap:olap}. In this chapter, we investigate the effects of light-weight compression methods by implementing dictionary encoding, bitpacking, and property packing. Also, we inquire into a new technique for compressing null pointers, which we denote as \textit{null-pointer compression}. \todo{See if this name can be used}

This chapter forms the third and final iteration where the goal is to reduce memory. There is one more iteration, but this does not seek to reduce memory any further, only utilize the storage structures to increase performance for read-intense operations.

\clearpage

\section{Introduction}
\label{sec:Introduction}
In Section \ref{sec:Compression}, we saw that compression not only reduces memory requirements but also comes with the benefit of increased performance. Compression is beneficial because it increases cache locality, turns processes CPU-bound from memory-bound and thus reduces the number of CPU cycles. SIMD instructions might also increase processing throughput on compressed data. According to Abadi \ea, compression increases performance by a factor of two on average \cite{Abadi2008-dd}. For compression to be beneficial the compression must be \textit{light-weight} since the real benefit of compression can only be leveraged if the decompression effort is minimized.

One such light-weight compression technique, is \textit{dictionary encoding}, which we saw in Section \ref{sec:Dictionary Encoding}. In a dictionary encoded column, each distinct value is stored once in a structure known as a dictionary, and instead of storing the actual values, keys to this dictionary are stored in the column. Not only does this technique save space, but it may increase query performance since comparisons are reduced to cheap integer operations. In this iteration, we extend our column store to support dictionary encoding and hope to leverage the benefits this technique gives us.

The second technique we implement in this iteration is bitpacking, which we studied in Section \ref{sec:Bitpacking}. In this compression scheme, no more bits than needed are used to store a value. This improves cache hit-rate and enables SIMD query processing. Bitpacking and dictionary encoding are commonly used in conjunction, and this what we will do in this iteration.

Last, we explore two techniques to save even more memory in \gap. The first is a technique we denote as \textit{null-pointer compression}. It is based on the observation that a \cn{CompositionObject} has several properties, properties that are commonly \nil. Thus, by moving the properties to the column store, we may save space by not allocating any values unless they are set. The second technique we apply is to compress the residual properties in a \cn{packed record}.

\section{Implementation}
\label{sec:Implementation}
In this section, we explain how compression is implemented by creating new \cn{FieldValueCollectionBase} subclasses and modifying the column store. We start with dictionary encoding and bitpacking. Then we proceed with null-pointer compression and property packing.

\subsection{Dictionary Encoding}
\label{sub:Dictionary Encoding}
% Short introduction on DE
In Section \ref{sec:Dictionary Encoding}, we saw that dictionary encoding is commonly used to store data in a database, especially read-optimized ones. Such encoding has several benefits, with the main being compression. Some operations are also speeded up by this storage format, especially string predicates. To do a select operation on a dictionary encoded column, the dictionary needs only to be scanned once to find the keys corresponding to the query, then the integer array is scanned. This reduces expensive string operations into cheap integer comparisons.

In a dictionary encoded column, the dictionary can be structured either for read or write performance. For read operations, there is a lookup from a dictionary index and the actual value. For such operations, implementing the dictionary as a list would suffice, adding new keys to the end of the list as they appear. Hence, lookups happen in constant time. However, with this implementation, assuming that the dictionary is not sorted, a linear search through the dictionary is triggered every time there is a write operation to the column, because the correct dictionary key must be found. 

\afigure{img/dict-inverse.png}{Dictionary Encoding. There exists a constant time lookup mechanism not only from dictionary keys to values, but also a lookup mechanism from values to dictionary keys.}{fig:dict-inverse}{0.8}
Since data sources in \gap~can be used in many different places, some write-intense, others read-intense, we implement a dictionary that can do both read and write operations in constant time. This comes at the cost of increased memory; a mapping from values to keys must be maintained. This is depicted in Figure \ref{fig:dict-inverse}. For the value-to-key lookup, a dictionary is used, such that lookups can be performed in constant time. When \fn{Consolidate} is called, the inverse dictionary lookup is discarded to save memory. 

The integer array, or value buffer, uses the same growth strategy as previous implementations. Like \cn{FieldValueCollection} and \cn{PrimitiveFieldValueCollection}, buffers are doubled on index overflow. The \fn{Consolidate} resizes the key array to the correct size after the data source has been loaded.

\afigure{img/PrimitiveDictionaryFieldValueCollection.png}{\cn{PrimitiveDictionaryFieldValueCollection} class diagram. The class inherits from \cn{FieldValueCollectionBase}.}{fig:PrimitiveDictionaryFieldValueCollection}{1.0}
We chose to implement the dicitonary and inverse lookup using \cn{TArray} and \cn{TDictionary} respectively. We also chose to store values as their primitive implementation, like we investigated in Chapter \ref{chap:storage-format}. The class inherits from \cn{FieldValueCollectionBase}, and uses the same primitive value helper delegate class as \cn{PrimitiveDictionaryFieldValueCollection}.

\subsection{Bitpacking}
\label{sub:Bitpacking}
% Short introduction on bp
Bitpacking is a compression technique where values are stored with no more bits than needed. As we saw in Section \ref{sub:Dictionary Encoding and Bitpacking}, dictionary encoding and bitpacking goes hand in hand. We, therefore, implement bitpacking in our dictionary encoded columns to save memory.

% Main implementation
\afigure{img/bp-overflow.png}{Bitpacking overflow. First, the value buffer is rebuilt to accomodate a new key in the dictionary that caused an overflow. Here, the array contents are copied to another buffer with extra padding. Second, if needed, a new cell is added for the new value.}{fig:bp-overflow}{1.0}
The value buffer is implemented as an array of 64-bits values which each value is denoted as a \textit{cell}. Unlike previous column implementations, a cell is added one at a time, instead of the doubling strategy. This was done for simplicity reasons. Depending on the number of bits used per value, this means that more values can be added per reallocation. On dictionary overflow, the new number of bits per value is calculated, and the entire value buffer is rebuilt. Both buffer and dictionary overflow is depicted in Figure \ref{fig:bp-overflow}. Like the non-packed version, \cn{PrimitivePackedDictionaryFieldValueCollection} uses an inverse lookup which are discarded on \fn{Consolidate}.

% Dictionary overflow
One of the main challenges with bitpacking, is word alignment. If keys are allowed to cross word boundaries, not only performance might degrade, but it increase implementation and code complexity. Therefore, we take a simpler approach, where the number of bits used to store each value must be a power of 2. As seen in Figure \ref{fig:bp-overflow}, a dictionary overflow causes the bits per value to go from two to four to avoid values stored across word boundaries.

\afigure{img/PrimitivePackedDictionaryFieldValueCollection.png}{\cn{PrimitivePackedDictionaryFieldValueCollection} class diagram. The class is very similar to \cn{PrimitiveDictionaryFieldValueCollection}, but it has a different storage structure for its values.}{fig:PrimitivePackedDictionaryFieldValueCollection}{0.7}
The bitpacked column class was implemented using a \cn{TArray<UInt64>} for storing data. Like the non-packed version, a \cn{TArray} was used for the dictionary and \cn{TDictionary} for the inverse lookup. In addition to this, it holds some state regarding the bitpacking, and a private function \fn{IncreaseDictionaryCapacity} which rebuilds the value buffer on dictionary overflow. The class diagram is seen in Figure \ref{fig:PrimitivePackedDictionaryFieldValueCollection}.

\missingfigure{Figur om hvordan man henter ut verdier med masken?}

\subsection{Null-Pointer Compression}
\label{sub:Null-Pointer Compression}
\afigure{img/null-pointer-compression.png}{Null-pointer compression. Based on the observation that most \cn{CompositionObject} properties are normally \texttt{null}, data can be compressed, since columns are not created before the properties are set.}{fig:null-pointer-compression}{0.7}
So far, we have compressed data from the Information System and Information System Development Layers. However, there are several other structures used by \cn{CompositionObject}s which are used in the Method Engineering layer. These structures include lists of data validation and integrety errors, formatting rules, and more. These attributes are not fundamentally different than attributes from the other model-driven engineering layers, which means we are able to put these pointers into our column store. Although one might initially think that we have only moved the problem, and that the pointers now are stored in the column store instead of at object-level, the compression is based on the observation that these pointers are normally \texttt{null}. This means that these columns does not need to be allocated before a value is actually set. An illustration of this is seen in Figure \ref{fig:null-pointer-compression}.

To enable null pointer compression, we implement a new generic column structure named \cn{DefaultObjectList<TType>}. This is an array structure that can be instantiated with any class type, that will return \texttt{nil} if no values are set. If values are set, the array is allocated. The \cn{CompositionValueCollection} are extended with such attributes for all properties needed in the \cn{CompositionObject}s with corresponding get and set methods that requires datasource index as input.

\subsection{Property Packing}
\label{sub:Property Packing}
\begin{delphicode}{Structure with packed data.}{lst:packed-struct}
PackedComObjData = packed record
    modifyCount,
    datasourceIndex    : integer; 
    state              : EObjectState;
    filterStatus       : EFilterStatus;
end;
\end{delphicode}
Some properties on a \cn{CompositionObject} are always set. This include a modified counter, some state variables, and of course the data source index. To reduce the memory footprint used by these attributes, they are packed in a struct using \delphi's \fn{packed} keyword. See Listing \ref{lst:packed-struct}. This packs the data and tells the compiler to disregard word boundaries \cite{noauthor_undated-vu}.

For this to work, the attributes is removed from the \cn{CompositionObject} class and moved to a \cn{PackedComObjData} record type. All read and write properties in \cn{CompositionObject} is rewritten to access this new structure.

After applying both null-pointer compression from Section \ref{sec:Null-Pointer Compression}, a \cn{CompositionObject} has no attributes except for a pointer to the column storage and a packed struct with essential state data. \todo{Vet dette ikke er helt sant, men kan jeg skrive det?}

\section{Results}
\label{sec:Results}
We test the changes from this iteration with Benchmark \ref{bm:q1} and Benchmark \ref{bm:write}. Like the previous two iterations, we are curious to see how compression techniques affect memory consumption, load time, read performance, and write performance. 

\begin{table}
    \begin{tabularx}{\textwidth}{X | X X}
        & Distinct Values & Dictionary Encoding? \\ 
        \hline
        \hline
        LINEITEMKEY & 600,000 & No \\
        LINESTATUS & 3 & Yes \\
        RETURNFLAG & 3 & Yes \\
        SHIPDATE & 2526 & Yes \\
        QUANTITY & 50 & Yes \\
        EXTENDEDPRICE & 598,966 & No \\
        DISCOUNT & 11 & Yes \\
        TAX & 9 & Yes
    \end{tabularx}
    \caption{Column selection for Benchmark \ref{bm:q1}. Low-cardinality columns are dictionary encoded, while the others are not. The numbers are based on Scaling Factor 0.1.}
    \label{tab:column-selection}
\end{table}

Like we read in Section \ref{sec:Dictionary Encoding}, dictionary encoding is only effective when there is a small number of distinct values compared to the number of total values. Hence, not all columns in the \textit{TPC-H Q1 Data Load Benchmark} should be encoded with dictionary encoding. Therefore, we picked certain \lineitem~properties by hand to be dictionary encoded, based on the column cardinality in the \lineitem~table in the \tpch. The dictionary encoded columns with cardinalities are shown in Table \ref{tab:column-selection}, and indicates that most columns benefit from dictionary encoding.

In this iteration, we test four different configurations: Dictionary encoding, dictionary encoding with raw value load, dictionary encoding with bitpacking, and dictionary encoding with bitpacking, null-pointer compression and property packing. For Benchmark \ref{bm:q1}, we test both SF0.01 and SF0.1.


\subsection{TPC-H Q1 Data Load Benchmark}
Benchmark \ref{bm:q1}, the \textit{TPC-H Q1 Data Load Benchmark}, was run with all three compression configurations. In addition to this, dictionary encoding was run with raw value load, as explained in Section \ref{sec:Avoiding GValues at Load Time}. Both scaling factors were used for the tests. Like the previous chapters, all tests were run three times per configuration. There was low variance in the results, and no single measurement was more than 15 \% different from the average value.
\begin{table}
    \centering
    \begin{tabularx}{\textwidth}{X | X X}
        & SF0.01 & SF0.1 \\ 
        \hline
        \hline
        Primitive Column Store & 333 bytes & 374 bytes \\
        Dictionary Encoding & 298 bytes & 318 bytes \\
        Dictionary Encoding w/ raw value load & 298 bytes & 318 bytes \\
        Dictionary Encoding w/ bitpacking & 290 bytes & 301 bytes \\
        Dictionary Encoding w/ bitpacking, null-pointer compression, and attribute packing & 226 bytes & 237 bytes \\
    \end{tabularx}
    \caption{Bytes per \texttt{LINEITEM} for various compression implementations.} 
    \label{tab:dict-bpl}
\end{table}

We see in Table \ref{tab:dict-bpl} that memory used per lineitem is reduced by 11 \% and 15 \% for scaling factors 0.01 and 0.1 respectively. Also, there is no longer a difference between memory footprint for original and new raw value loading mechanism, something we observed in Section \ref{sub:storage-format-tpch-results}. There is a slight memory reduction by applying bit-packing, but null-pointer compression and attribute packing has reduced memory storage from 374 to 237 bytes for scaling factor 0.1, which means our compression has reduced memory usage by 37 \%. 
\begin{table}
    \centering
    \begin{tabularx}{\textwidth}{X | X X}
        & SF0.01 & SF0.1 \\ 
        \hline
        \hline
        Primitive Column Store & 1210 ms & 13585 ms \\
        Dictionary Encoding & 1332 ms & 14826 ms \\
        Dictionary Encoding w/ raw value load & 2215 ms & 21760 ms \\
        Dictionary Encoding w/ bitpacking & 1421 ms &  15366 ms \\
        Dictionary Encoding w/ bitpacking, null-pointer compression, and attribute packing & 1389 ms & 15584 ms \\
    \end{tabularx}
    \caption{Load times for Benchmark \ref{bm:q1} for different column compression implementations for scaling factors 0.01 and 0.1.} 
    \label{tab:dict-load}
\end{table}

As we see in Table \ref{tab:dict-load}, load times have increased as a result of the compression. For scaling factor 0.1, dictionary encoding adds 1.2 seconds load time, bitpacking adds an additional 0.5 seconds, and null-pointer compression and attribute packing adds 0.2 seconds more. Even though load time has been increased, it is still less than the original implementation. Loading raw values into the columns increases load time to 21.8 seconds for SF 0.1. We discuss this observation in Section \ref{sec:compression-discussion}.

We refrain from presenting the timing results for lookup index generation and source measure lookup from Benchmark \ref{bm:q1}. These does not vary significantly from the results presented in Section \ref{sec:storage-format-test-results}.

\subsection{Write Benchmark}
\label{sub:compression-write-benchmark}
\begin{table}
    \begin{tabularx}{\textwidth}{X | X X X X}
         & \texttt{QUANTITY} & \texttt{EXTENDEDPRICE} & \texttt{COMMENT} & \texttt{SHIPDATE}\\ 
        \hline
        \hline
        Primitive Column Store & 1807 ms & 1779 ms & 1820 ms & 2352 ms \\
        Dictionary Encoding & 1855 ms & 1774 ms & 1753 ms & 2183 ms \\
        Dictionary Encoding /w Bitpacking & 1757 ms & 1638 ms & 1657 ms & 2198 ms \\
        Dictionary Encoding /w Bitpacking, null-pointer compression and attribute packing & 1939 ms & 1813 ms & 1777 ms & 227 ms \\
    \end{tabularx}
    \caption{Test results for Benchmark \ref{bm:write}.}
    \label{tab:compression-write}
\end{table}
Write performance has not been significantly changed compared to primitive column storage, as seen it Table \ref{tab:compression-write}. Bitpacking is the fastest of all implementations, but only by very little.

\section{Discussion}
\label{sec:compression-discussion}
We see that dictionary encoding is successful for compressing data, and for SF0.1 in the \textit{TPC-H Q1 Data Load Benchmark}, memory is reduced by 15 \% compared to the primitive column structure from the previous chapter. Bitpacking also contributes to reduced memory footprint, but not more than five percent. The increased compression rates come at the cost of higher load times and is now up to 15,4 seconds to load the entire mart into memory.

Null-pointer compression and property packing contribute to large memory savings and reduces the storage per \lineitem~from 301 bytes to 237 bytes. We observe that this number makes sense. We removed a total of eight pointers, and with 8 bytes per pointer, 64 bytes are saved. Hence, we are unsure whether property packing had any effect at all. \todo{check how many removed pointers} Either way, these two techniques does not significantly increase load time.

Neither of the applied techniques has changed the write performance, which strengthens our hypothesis that write operations have much overhead associated integrity, data validation, and formula calculation. However, read-intense operations in the \textit{TPC-H Q1 Data Load Benchmark} still suffers severely from the modification applied to \gap~so far, and we believe this is caused by the \cn{GValue} interface.

If we load raw XML string values directly into a dictionary encoded column, we observe that we save the same amount of memory as the built-in caching mechanism in \gap. In other words, the number of bytes for dictionary encoding and dictionary encoding with raw value load is the same. However, the load time is still significantly longer loading raw values. We believe this is because the caching mechanism not only re-uses data but also stores string-to-primitive conversion results. We discuss how a similar caching mechanism can be implemented in our column store in Section \ref{compression:future-work}

\section{Iteration Conclusion}
\label{sec:Iteration Conclusion}
\afigure{img/bytes.png}{The bytes used per \lineitem~during the course of this research.}{fig:bytes}{1.0}
\afigure{img/load.png}{The number of milliseconds it takes to load the \textit{TPC-H Q1} data mart for the course of this research.}{fig:load}{1.0}
In this iteration, we have reduced the memory required to load the \textit{TPC-H Q1} mart by 37 \%. This reduction is mainly caused by dictionary encoding and null-pointer compression. Compared to the original \gap~implementation, the bytes per \lineitem~is reduced by 67 \%. The reduction in memory usage is illustrated in Figure \ref{fig:bytes}. Although the introduction of primitive data types and compression has increased load time from the first \cn{FieldValueCollection} implementation, the time it takes to load the \textit{TPC-H Q1} mart is still reduced by 33 \% compared to the original implementation. This is illustrated in Figure \ref{fig:load}.

Concluding this iteration, we are left with two major challenges. The first challenge was discovered in the previous iteration: Read-intensive operations must be adapted to utilize the new storage format. For instance, one of source measure lookup operations in Benchmark \ref{bm:q1}~now takes 6.9 seconds, an operation which previously took 0.3 seconds. We address this challenge in Chapter \ref{chap:operations}.

The second challenge, which is a result of dictionary compression, is to select the correct column format for different object class properties. In this iteration, we picked columns that would benefit from dictionary encoding by hand. With this approach, the \gap~expert users need knowledge about data cardinality, which is, in our opininion, knowledge they should not need to have. In Chapter \ref{chap:misc}, we investigate how \gap~can pick the correct storage format without modeler intervention by using database provided statistics.

\subsection{Future Work}
\label{compression:future-work}
Like the last iteration, we encourage studying the built-in caching mechanism in \gap. We have proved that dictionary encoded columns re-use values as efficiently as the existing mechanism, but if caching is disabled, the load time is significantly higher. We believe this is caused by the caching mechanism re-using string-to-primitive value conversions. One straight-forward solution to the problem would be to add a \vn{rawInverseLookup} to the \cn{PrimitiveDictionaryFieldValueCollection} class which translates raw XML strings to dictionary indexes.

In our bitpacking implementation, cells are added one at a time to a column instead of doubling capacity. Although one cell normally contains more than one value, the effects of the doubling growth strategy should be investigated.
